{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSCI 573 - Feature and Model Selection\n",
    "\n",
    "# Lab 2: Feature engineering, feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "- [Submission instructions](#si)\n",
    "- [Exercise 1: Feature engineering](#1)\n",
    "- [(optional) Exercise 2: Change of basis](#2)\n",
    "- [Exercise 3: Recursive feature elimination and forward selection](#3)\n",
    "- [(optional) Exercise 4: Implement forward selection](#4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission instructions <a name=\"si\"></a>\n",
    "<hr>\n",
    "rubric={mechanics:2}\n",
    "\n",
    "You will receive marks for correctly submitting this assignment. \n",
    "\n",
    "To correctly submit this assignment follow the instructions below:\n",
    "\n",
    "- Push your assignment to your GitHub repository. \n",
    "- Add a link to your GitHub repository here: LINK TO YOUR GITHUB REPO \n",
    "- Upload an HTML render of your assignment to Canvas. The last cell of this notebook will help you do that.\n",
    "- Be sure to follow the [general lab instructions](https://ubc-mds.github.io/resources_pages/general_lab_instructions/).\n",
    "\n",
    "[Here](https://github.com/UBC-MDS/public/tree/master/rubric) you will find the description of each rubric used in MDS.\n",
    "\n",
    "**NOTE: The data you download for use in this lab SHOULD NOT BE PUSHED TO YOUR REPOSITORY. You might be penalised for pushing datasets to your repository. I have seeded the repository with `.gitignore` and hoping that it won't let you push CSVs.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "%matplotlib inline\n",
    "import string\n",
    "from collections import deque\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# data\n",
    "from sklearn import datasets\n",
    "from sklearn.compose import ColumnTransformer, make_column_transformer\n",
    "from sklearn.dummy import DummyClassifier, DummyRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Feature selection\n",
    "from sklearn.feature_selection import RFE, RFECV\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# classifiers / models\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression, Ridge, RidgeCV\n",
    "\n",
    "# other\n",
    "from sklearn.metrics import accuracy_score, log_loss, make_scorer, mean_squared_error\n",
    "from sklearn.model_selection import (\n",
    "    GridSearchCV,\n",
    "    RandomizedSearchCV,\n",
    "    ShuffleSplit,\n",
    "    cross_val_score,\n",
    "    cross_validate,\n",
    "    train_test_split,\n",
    ")\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.preprocessing import (\n",
    "    OneHotEncoder,\n",
    "    OrdinalEncoder,\n",
    "    PolynomialFeatures,\n",
    "    StandardScaler,\n",
    ")\n",
    "from sklearn.svm import SVC, SVR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Feature engineering <a name=\"1\"></a>\n",
    "<hr>\n",
    "\n",
    "One of the most important aspects which influences performance of machine learning models is the features used to represent the problem. If your underlying representation is bad whatever fancy model you use is not going to help and with good representation, a simple and interpretable model is likely to perform reasonably well. \n",
    "\n",
    "**Feature engineering** is the process of transforming raw data into features that better represent the underlying problem to the predictive models. \n",
    "\n",
    "In this exercise we'll engineer our own features on [the Disaster Tweets dataset](https://www.kaggle.com/vstepanenko/disaster-tweets). \n",
    "\n",
    "Note that coming up with features is difficult, time-consuming, and requires expert knowledge. The purpose of this exercise is to give you a little taste of feature engineering, which you are likely to be doing in your career as a data scientist or a machine learning practitioner. In this exercise, since we'll be using simplistic features, you might not get better scores with your engineered features, and that's fine. The purpose here is to make you familiar with the process of feature engineering rather than getting the best scores. \n",
    "\n",
    "As usual, download the dataset, unzip it and save it in your lab folder. Do not push it into the repository. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN STARTER CODE\n",
    "\n",
    "df = pd.read_csv(\"tweets.csv\", usecols=[\"keyword\", \"text\", \"target\", \"location\"])\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=2)\n",
    "train_df.head()\n",
    "\n",
    "### BEGIN STARTER CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Preliminary analysis\n",
    "rubric={reasoning:5}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. State in your own words what problem you are trying to solve here. (One sentence is enough.) \n",
    "2. Do you have class imbalance. If yes, do we need to deal with it? What metric would be appropriate in this case? \n",
    "3. I am defining `text_feature` and `target` in the starter code below. Identify other feature types and the transformations you want to apply on features. Note that \"location\" feature could be a potentially useful feature but is a bit complicated to encode and we are going to exclude it in this assignment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN STARTER CODE\n",
    "\n",
    "text_feature = \"text\"\n",
    "target = \"target\"\n",
    "\n",
    "X_train, y_train = train_df.drop(columns=[\"target\", \"location\"]), train_df[target]\n",
    "X_test, y_test = test_df.drop(columns=[\"target\", \"location\"]), test_df[target]\n",
    "\n",
    "### END STARTER CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**solution_1_1_1**\n",
    "\n",
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**solution_1_1_2**\n",
    "\n",
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solution_1_1_3\n",
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (optional) 1.2 \n",
    "rubric={reasoning:1}\n",
    "\n",
    "**Your tasks:**\n",
    "1. Here we are dropping the `location` feature. How you might encode it if you decide to include it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**solution_1_2_1**\n",
    "\n",
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 DummyClassifier\n",
    "rubric={accuracy:1}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Report cross-validation mean f1 score and accuracy for `DummyClassifier` `with strategy=\"stratified\"`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solution_1_3_1\n",
    "\n",
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 `CountVectorizer`\n",
    "rubric={accuracy:3,reasoning:1}\n",
    "\n",
    "So far for text data, we have been using bag of words representation as features. \n",
    "Let's examine the scores with a bag of words model. \n",
    "\n",
    "**Your tasks:**\n",
    "1. Define a pipeline with `CountVectorizer` on the \"text\" column and `LogisticRegression` classifier. Set `max_features` of `CountVectorizer` to 20_000 and `class_weight` of `LogisticRegression` to \"balanced\". (These are  optimized hyperparameter values. We won't carry out hyperparameter optimization here in the interest of time.)\n",
    "2. Report mean cross-validation f1 score and accuracy. Compare it with the baseline model in 1.3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solution_1_4_1\n",
    "\n",
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solution_1_4_2\n",
    "\n",
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**solution_1_4_2 (reasoning)**\n",
    "\n",
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Include _keyword_ feature\n",
    "rubric={accuracy:4,reasoning:1}\n",
    "\n",
    "The _keyword_ feature seems relevant for predicting whether the tweet is disastrous or not. \n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Build a column transformer for transforming _keyword_ feature and _text_ feature. Set `max_features` of `CountVectorizer` to 20_000. So far you haven't used `CountVectorizer` with other transformations. Below is an example column transformer which shows how to use `CountVectorizer` with other transformers. Unlike transformers such as `StandardScaler()` for numeric features, for `CountVectorizer` transformer, you pass the feature name as a string rather than a list of features. (So if you have multiple text columns, you'll have to define multiple `CountVectorizer` transformers.) \n",
    "```\n",
    "preprocessor = make_column_transformer(\n",
    "    (StandardScaler(), numeric_features), # scale for numeric features\n",
    "    (CountVectorizer(), \"text\") # bag of words for text feature\n",
    ")\n",
    "```\n",
    "2. Build a pipeline with the column transformer in 1. and `LogisticRegression` classifier. Set `class_weight` of `LogisticRegression` to \"balanced\". \n",
    "3. Report mean cross-validation f1 scores and accuracy. \n",
    "4. Are you getting better scores than 1.3 and 1.4? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solution_1_5_1\n",
    "\n",
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solution_1_5_2\n",
    "\n",
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solution_1_5_3\n",
    "\n",
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**solution_1_5_4**\n",
    "\n",
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.6: Adding new features\n",
    "rubric={reasoning:5}\n",
    "\n",
    "Is it possible to further improve the scores? How about adding new features based on our intuitions? \n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Name 3 to 4 additional features you think would be helpful in predicting the target. An example would be a binary feature \"has_emoticons\" indicating whether the tweet has emoticons or not. Explain your intuition behind the features and discuss how hard in would be to engineer these features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**solution_1_6_1**\n",
    "\n",
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.7: Extracting your own features \n",
    "rubric={accuracy:4,reasoning:4}\n",
    "\n",
    "In this exercise, we will be adding some very basic length-related and sentiment features.  \n",
    "\n",
    "You will need to install a popular library called `nltk` for this exercise. For that, run the following commands in your `conda` environment. \n",
    "\n",
    "```\n",
    "conda install -c anaconda nltk \n",
    "nltk.download(\"vader_lexicon\")\n",
    "nltk.download(\"punkt\")\n",
    "```        \n",
    "\n",
    "Run the starter code below creates three new features: \n",
    "- Relative character length in the tweet. \n",
    "- Number of words in the tweet.\n",
    "- Sentiment of the tweet (positive (pos), negative (neg), neutral (neu), compound (mixture of different sentiments)). In 571, you carried out sentiment analysis on the IMDB data set. Here we are using some pre-trained machine learning model to extract sentiment expressed in the tweets. \n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Extract at least two more features that you think might be relevant for prediction and store them as new columns in the train and test sets. Briefly explain your intuition on why these features might help the prediction task. \n",
    "2. Would it have been OK to create new columns directly in the original `df` instead of creating them separately for train and test splits? Would that be violation of the golden rule? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN STARTER CODE\n",
    "\n",
    "import nltk\n",
    "\n",
    "nltk.download(\"vader_lexicon\")\n",
    "nltk.download(\"punkt\")\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "### END STARTER CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN STARTER CODE\n",
    "\n",
    "\n",
    "def get_relative_length(text, TWITTER_ALLOWED_CHARS=280.0):\n",
    "    \"\"\"\n",
    "    Returns the relative length of text.\n",
    "\n",
    "    Parameters:\n",
    "    ------\n",
    "    text: (str)\n",
    "    the input text\n",
    "\n",
    "    Keyword arguments:\n",
    "    ------\n",
    "    TWITTER_ALLOWED_CHARS: (float)\n",
    "    the denominator for finding relative length\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    relative length of text: (float)\n",
    "\n",
    "    \"\"\"\n",
    "    return len(text) / TWITTER_ALLOWED_CHARS\n",
    "\n",
    "\n",
    "def get_length_in_words(text):\n",
    "    \"\"\"\n",
    "    Returns the length of the text in words.\n",
    "\n",
    "    Parameters:\n",
    "    ------\n",
    "    text: (str)\n",
    "    the input text\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    length of tokenized text: (int)\n",
    "\n",
    "    \"\"\"\n",
    "    return len(nltk.word_tokenize(text))\n",
    "\n",
    "\n",
    "def get_sentiment(text):\n",
    "    \"\"\"\n",
    "    Returns the maximum scoring sentiment of the text\n",
    "\n",
    "    Parameters:\n",
    "    ------\n",
    "    text: (str)\n",
    "    the input text\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    sentiment of the text: (str)\n",
    "    \"\"\"\n",
    "    scores = sid.polarity_scores(text)\n",
    "    return max(scores, key=lambda x: scores[x])\n",
    "\n",
    "\n",
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN STARTER CODE\n",
    "\n",
    "train_df = train_df.assign(n_words=train_df[\"text\"].apply(get_length_in_words))\n",
    "train_df = train_df.assign(sentiment=train_df[\"text\"].apply(get_sentiment))\n",
    "train_df = train_df.assign(rel_char_len=train_df[\"text\"].apply(get_relative_length))\n",
    "\n",
    "test_df = test_df.assign(n_words=test_df[\"text\"].apply(get_length_in_words))\n",
    "test_df = test_df.assign(sentiment=test_df[\"text\"].apply(get_sentiment))\n",
    "test_df = test_df.assign(rel_char_len=test_df[\"text\"].apply(get_relative_length))\n",
    "\n",
    "### END STARTER CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solution_1_7_1\n",
    "\n",
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**solution_1_7_1 (reasoning)**\n",
    "\n",
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**solution_1_7_2**\n",
    "\n",
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.8 Pipeline with all features\n",
    "rubric={accuracy:4,reasoning:2}\n",
    "\n",
    "**Your tasks:**\n",
    "1. Identify different feature types in your new data set with the features you created above, and separate features and targets from your new dataset. \n",
    "2. Define a column transformer for your mixed feature types. Again, set `max_features` of `CountVectorizer` to 20_000.  \n",
    "3. Define a pipeline with the column transformer and `LogisticRegression` with `class_weight` of `LogisticRegression` set to \"balanced\" and report mean cross-validation f1 scores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solution_1_8_1\n",
    "\n",
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solution_1_8_2\n",
    "\n",
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solution_1_8_3\n",
    "\n",
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.9 Interpretation\n",
    "rubric={accuracy:4,reasoning:2}\n",
    "\n",
    "1. Do you see any improvements with the new features compared to when you used only `CountVectorizer` features? Note that feature engineering is hard and requires domain expertise. If you do not see big improvements in scores with new features, that's OK. Do not get discouraged. The purpose of this exercise is to make you familiar to the process of extracting new features rather than getting the best scores. \n",
    "2. Show the first 20 coefficients with largest magnitudes and corresponding features. \n",
    "3. Examine the coefficients of the features we have extracted above. Do they make sense? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**solution_1_9_1**\n",
    "\n",
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solution_1_9_2\n",
    "\n",
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**solution_1_9_3**\n",
    "\n",
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.10 Test results\n",
    "rubric={accuracy:2, reasoning:2}\n",
    "\n",
    "**Yout tasks**\n",
    "\n",
    "1. Report f1 score on the test set with the model trained with all features. \n",
    "2. What additional time, other than prediction time, do we need if we are to use this model with our engineered features on the deployment data?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solution_1_10_1\n",
    "\n",
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**solution_1_10_2**\n",
    "\n",
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset for next exercises\n",
    "<hr>\n",
    "\n",
    "In the following exercises, we'll be using [`sklearn`'s boston housing dataset](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_boston.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN STARTER CODE\n",
    "\n",
    "from sklearn.datasets import load_boston\n",
    "\n",
    "boston_housing = load_boston()\n",
    "print(boston_housing.keys())\n",
    "print(boston_housing.DESCR)\n",
    "\n",
    "### END STARTER CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN STARTER CODE\n",
    "\n",
    "boston_df = pd.DataFrame(boston_housing.data, columns=boston_housing.feature_names)\n",
    "boston_df[\"target\"] = boston_housing.target\n",
    "train_df, test_df = train_test_split(boston_df, test_size=0.2, random_state=2)\n",
    "\n",
    "X_train, y_train = train_df.drop(columns=[\"target\"]), train_df[\"target\"]\n",
    "X_train, y_test = train_df.drop(columns=[\"target\"]), train_df[\"target\"]\n",
    "\n",
    "### END STARTER CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (optional) Exercise 2: Change of basis <a name=\"2\"></a>\n",
    "<hr>\n",
    "\n",
    "The linear model is problematic when the target is a non-linear function of the input. With high dimensional data we cannot really know whether the target is a linear or non-linear function of the input. One way to examine this is by using _polynomial features_. Suppose you have a single feature $x_1$ in your original data, you can think of transforming the data into the following matrix $X_{poly}$ where each of its rows contains the values $(X_{i})^j$ for $j=0$ up to some maximum $degree$. E.g., \n",
    "\n",
    "$$\n",
    "X_{poly} = \\left[\\begin{array}{cccc}\n",
    "1 & x_1 & (x_1)^2 & (x_1)^3\\\\\n",
    "1 & x_2 & (x_2)^2 & (x_2)^3\\\\\n",
    "\\vdots\\\\\n",
    "1 & x_n & (x_n)^2 & (x_N)^3\\\\\n",
    "\\end{array}\n",
    "\\right],\n",
    "$$\n",
    "\n",
    "We can then fit a least squares model as if the above were our data set. You can think of this as \"changing the model by changing the data\" since we are still using a linear model but making the fit nonlinear by inventing new features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (optional) 2.1 Polynomial feature transformations  \n",
    "rubric={reasoning:1}\n",
    "\n",
    "**Your tasks:**\n",
    "1. Is it possible to visualize the our Boston housing data and examine whether a linear fit is good fit for this dataset or not? \n",
    "2. Carry out cross-validation using `DummyRegressor` on the train portion. \n",
    "3. Define a pipeline with `PolynomialFeatures` and `RidgeCV`. \n",
    "4. Examine the train and validation scores for three values for `degree` hyperparameter of `PolynomialFeatures`: 1, 2, and 3. Use either negative MAPE or `neg_root_mean_squared_error` for scoring. \n",
    "5. Which value of `degree` is giving you the best results? How many new features do you have with this degree?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**solution_2_1_1**\n",
    "\n",
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solution_2_1_2\n",
    "\n",
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solution_2_1_3\n",
    "\n",
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solution_2_1_4\n",
    "\n",
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solution_2_1_5\n",
    "\n",
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Feature importances and feature selection <a name=\"3\"></a>\n",
    "<hr>\n",
    "\n",
    "In this exercise we'll explore feature importances, recursive feature elimination, adding polynomial features, and forward selection. You could use the scoring method of your choice. The default $R^2$ is fine too.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.1 Adding random noise\n",
    "rubric={reasoning:2}\n",
    "\n",
    "The following code shows the coefficients learned by `RidgeCV` on the Boston housing dataset. It then adds a column of random noise to `X_train` and re-trains and examines the coefficients again. We see that the model has assigned a non-zero coefficient to the noise feature. But wait, we know this feature can't possibly be useful. \n",
    "\n",
    "**Yout taks:**\n",
    "\n",
    "1. why is the importance of the random noise feature non-zero (and in fact larger than for some real features)? Maximum 2 sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrcv = RidgeCV()\n",
    "lrcv.fit(X_train, y_train)\n",
    "pd.DataFrame(data=lrcv.coef_, index=X_train.columns, columns=[\"coefficient\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_noise = np.random.randn(X_train.shape[0], 1)\n",
    "X_train_noise = pd.concat(\n",
    "    (X_train, pd.DataFrame(random_noise, columns=[\"noise\"], index=X_train.index)),\n",
    "    axis=1,\n",
    ")\n",
    "X_train_noise.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrcv = RidgeCV()\n",
    "lrcv.fit(X_train_noise, y_train)\n",
    "pd.DataFrame(data=lrcv.coef_, index=X_train_noise.columns, columns=[\"coefficient\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**solution_3_1_1**\n",
    "\n",
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 `RFECV` \n",
    "rubric={accuracy:4,reasoning:2}\n",
    "\n",
    "In this exercise, you'll explore recursive feature elimination for feature selection. \n",
    "\n",
    "**Your tasks:**\n",
    "1. Define a pipeline with the following steps and report mean cross-validation scores with the pipeline on the Boston housing dataset.  \n",
    "    - `StandardScaler` with default parameters\n",
    "    - `RidgeCV`\n",
    "2. Now add `RFECV` with `Ridge` to the pipeline and report mean cross-validation scores with the pipeline.     \n",
    "3. Why are we using `RFECV` and `RidgeCV` in the pipeline? \n",
    "4. How many features have been selected by the `RFECV`. You can access this using `n_features_` attribute of the `RFECV` object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# soliution_3_2_1\n",
    "\n",
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**soliution_3_2_3**\n",
    "\n",
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# soliution_3_2_4\n",
    "\n",
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**soliution_3_2_4**\n",
    "\n",
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3: `PolynomialFeatures` + [`RFECV`](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFECV.html)\n",
    "rubric={accuracy:3,reasoning:3}\n",
    "\n",
    "**Your tasks:**\n",
    "1. Add one more step to the pipeline above: **`PolynomialFeatures()`** \n",
    "2. Carry out cross-validation using the pipeline, and report the mean validation scores. \n",
    "3. What's the effect of adding `PolynomialFeatures` step in the pipeline? How many total features there will be after applying `PolynomialFeatures` transformation? How many features have been selected by `RFECV`?\n",
    "Are you getting better scores compared to 3.2? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solution_3_3_1\n",
    "\n",
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solution_3_3_2\n",
    "\n",
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**solution_3_3_3**\n",
    "\n",
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solution 3_3_3\n",
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4: `PolynomialFeatures` + Forward selection\n",
    "rubric={accuracy:4,reasoning:2}\n",
    "\n",
    "Forward selection is not implemented in `sklearn`. But it is in a package `mlxtend`, which is compatible with `sklearn` pipelines. \n",
    "\n",
    "So first, let's install `mlxtend` in our environment. \n",
    "```\n",
    "conda install -c conda-forge mlxtend\n",
    "```\n",
    "\n",
    "**Your tasks:**\n",
    "1. Define a pipeline with forward search instead of `RFECV`. So add the following step in the pipeline instead of `RFECV` and report mean cross-validation scores. \n",
    "    - `SequentialFeatureSelector` with `Ridge`, and `k_features` = 20. \n",
    "2. Are you getting comparable scores? Is there any overlap between the features selected by `RFECV` and forward selection? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.feature_selection import SequentialFeatureSelector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solution_3_4_1\n",
    "\n",
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solution_3_4_2\n",
    "\n",
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**solution_3_4_2 (reasoning)**\n",
    "\n",
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5\n",
    "rubric={reasoning:2}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Discuss advantages and disadvantages of recursive feature elimination (RFE) and forward selection. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**solution_3_5_1**\n",
    "\n",
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (optional) Exercise 4: Implement forward selection <a name=\"4\"></a>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 (optional) Implement your own forward selection\n",
    "rubric={reasoning:2}\n",
    "\n",
    "**Your tasks:**\n",
    "1. Implement the `fit` method of the forward selection algorithm using the starter code below. This algorithm works iteratively. At each step, add in the feature that most reduces the validation error. Stop adding features once the validation error stops decreasing. Feel free to adapt the `init` and `transform` methods as you see appropriate. You are welcome to hard-code in a particular choice of model (e.g., `Ridge` or `SVR` with a linear kernel). Optionally, abstract away the model so that your forward selection function can be called with any model so long as it implements `fit` and `predict` and `score` like most `sklearn` models.\n",
    "2. Carry out feature selection using your method on the Boston housing dataset above. \n",
    "3. Discuss your results. Are you getting similar results to `mlxtend`? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ForwardSelection:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        min_features=None,\n",
    "        max_features=None,\n",
    "        scoring=None,\n",
    "        cv=None,\n",
    "        mode=\"regression\",\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initializes the ForwardSelection object\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        model -- sklearn regressor or classifier object\n",
    "            The sklearn regression or classification model\n",
    "\n",
    "        Keyword arguments\n",
    "        ----------\n",
    "        min_features -- (int)\n",
    "            the minimum number of features the model must select (default None)\n",
    "        max_features -- (int)\n",
    "            the maximum number of features that model may select (default None)\n",
    "        scoring -- (str)\n",
    "            the scoring that will be used for feature selection (default None)\n",
    "        cv -- (int)\n",
    "            the number of folds in the cross validation (default None)\n",
    "        mode -- (str)\n",
    "            Whether you are carrying out feature selection for regression or classification\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            None\n",
    "\n",
    "        \"\"\"\n",
    "        self.max_features = max_features\n",
    "        if min_features is None:\n",
    "            self.min_features = 1\n",
    "        else:\n",
    "            self.min_features = min_features\n",
    "\n",
    "        self.model = model\n",
    "        self.scoring = scoring\n",
    "        self.cv = cv\n",
    "        self.score_ = None\n",
    "        self.ftr_ = []\n",
    "        self.mode = mode\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Finds the best feature set using the `Forward Selection` algorithm.\n",
    "\n",
    "        Parameters:\n",
    "        -------\n",
    "        X -- (numpy array)\n",
    "            Feature vector\n",
    "        y -- (numpy array)\n",
    "            target vector\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            None\n",
    "\n",
    "        \"\"\"\n",
    "        # solution_4_1_1\n",
    "        ### YOUR ANSWER HERE\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Return the features selected using the `Forward Selection` algorithm.\n",
    "\n",
    "        Parameters:\n",
    "        -------\n",
    "        X -- (numpy array)\n",
    "            Feature vector\n",
    "        y -- (numpy array)\n",
    "            target vector (default = None)\n",
    "\n",
    "        Return:\n",
    "        -------\n",
    "        selected features from X\n",
    "\n",
    "        \"\"\"\n",
    "        return X[:, self.ftr_]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solution_4_1_2\n",
    "\n",
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**solution_4_1_3**\n",
    "\n",
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission to Canvas\n",
    "\n",
    "**PLEASE READ: When you are ready to submit your assignment do the following:**\n",
    "\n",
    "- Run all cells in your notebook to make sure there are no errors by doing Kernel -->  Restart Kernel and Run All Cells...\n",
    "- If you are using the \"573\" `conda` environment, make sure to select it before running all cells. \n",
    "- Convert your notebook to .html format using the `convert_notebook()` function below or by File -> Export Notebook As... -> Export Notebook to HTML\n",
    "- Run the code `submit()` below to go through an interactive submission process to Canvas.\n",
    "After submission, be sure to do a final push of all your work to GitHub (including the rendered html file)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from canvasutils.submit import convert_notebook, submit\n",
    "\n",
    "# convert_notebook(\"lab2.ipynb\", \"html\")  # uncomment and run when you want to try convert your notebook (or you can convert manually from the File menu)\n",
    "# submit(course_code=59091, token=False)  # uncomment and run when ready to submit to Canvas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations on finishing the lab!! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:571]",
   "language": "python",
   "name": "conda-env-571-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
