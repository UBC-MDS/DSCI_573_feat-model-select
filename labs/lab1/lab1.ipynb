{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSCI 573 - Feature and Model Selection\n",
    "\n",
    "## Lab 1: Evaluation metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "\n",
    "- [Submission guidelines](#sg)\n",
    "- [Exercise 0: Remembering important concepts from 571](#0)\n",
    "- [Exercise 1: Precision, recall, and f1 score by hand](#1)\n",
    "- [Exercise 2: Classification evaluation metrics using `sklearn`](#2)\n",
    "- [Exercise 3: Regresison metrics](#3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "from hashlib import sha1\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tests_lab1\n",
    "from sklearn import datasets\n",
    "from sklearn.compose import (\n",
    "    ColumnTransformer,\n",
    "    TransformedTargetRegressor,\n",
    "    make_column_transformer,\n",
    ")\n",
    "from sklearn.dummy import DummyClassifier, DummyRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression, Ridge\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    f1_score,\n",
    "    make_scorer,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    ")\n",
    "from sklearn.model_selection import (\n",
    "    GridSearchCV,\n",
    "    RandomizedSearchCV,\n",
    "    cross_validate,\n",
    "    train_test_split,\n",
    ")\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler\n",
    "from sklearn.svm import SVC, SVR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission instructions <a name=\"si\"></a>\n",
    "<hr>\n",
    "rubric={mechanics:2}\n",
    "\n",
    "You will receive marks for correctly submitting this assignment. \n",
    "\n",
    "To correctly submit this assignment follow the instructions below:\n",
    "\n",
    "- Push your assignment to your GitHub repository. \n",
    "- Add a link to your GitHub repository here: LINK TO YOUR GITHUB REPO \n",
    "- Upload an HTML render of your assignment to Canvas. The last cell of this notebook will help you do that.\n",
    "- Be sure to follow the [general lab instructions](https://ubc-mds.github.io/resources_pages/general_lab_instructions/).\n",
    "\n",
    "[Here](https://github.com/UBC-MDS/public/tree/master/rubric) you will find the description of each rubric used in MDS.\n",
    "\n",
    "**NOTE: The data you download for use in this lab SHOULD NOT BE PUSHED TO YOUR REPOSITORY. You might be penalised for pushing datasets to your repository. I have seeded the repository with `.gitignore` and hoping that it won't let you push CSVs.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 0: Remembering important concepts from 571 <a name=\"0\"></a>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.1 Overfitting \n",
    "rubric={reasoning:2}\n",
    "\n",
    "Why would it be a problem to set the `max_depth` hyperparameter of `DecisionTreeClassifier` purely based on training accuracy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.2 Parameters and hyperparameters\n",
    "rubric={reasoning:2}\n",
    "\n",
    "Explain the difference between parameters and hyperparameters. Which ones do we tend to tune with cross-validation? Why wouldn't cross validation be appropriate in the other case?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.3 Cross-validation\n",
    "rubric={reasoning:2}\n",
    "\n",
    "1. What is an advantage and a disadvantage of 20-fold cross-validation over 3-fold cross-validation? \n",
    "2. Assuming $n$ is the number of training examples, if you carry out $n$-fold cross-validation, how many validation example(s) there would be in each fold? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (optional) 0.4 \n",
    "rubric={reasoning:1}\n",
    "\n",
    "You have a (fictional) model that trains in $\\mathcal{O}(n^2d)$ time (total) and makes predictions in $\\mathcal{O}(d^2)$ time (per example), where $n$ is the number of examples and $d$ is the number of features. What is the time complexity of evaluating this model with $k$-fold cross-validation? Answer using big-O notation and explain or show your work. Your answer may depend on $n$, $d$, and/or $k$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Precision, recall, and f1 score by hand <a name=\"1\"></a>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the problem of predicting whether a patient has a disease or not. Below are confusion matrices of two machine learning models: Model A and Model B. \n",
    "\n",
    "- Model A\n",
    "|    Actual/Predicted      | Predicted disease | Predicted no disease |\n",
    "| :------------- | -----------------------: | -----------------------: |\n",
    "| **Actual disease**       | 2 | 8 |\n",
    "| **Actual no disease**       | 0 | 100 |\n",
    "\n",
    "\n",
    "- Model B\n",
    "|    Actual/Predicted      | Predicted disease | Predicted no disease |\n",
    "| :------------- | -----------------------: | -----------------------: |\n",
    "| **Actual disease**       | 6 | 4 |\n",
    "| **Actual no disease**       | 10 | 90 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Positive vs. negative class \n",
    "rubric={reasoning:2}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "Precision, recall, and f1 score depend crucially upon which class is considered \"positive\", that is the thing you wish to find. In the example above, which class is likely to be the \"positive\" class? Why? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Accuracy\n",
    "rubric={autograde:2}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "Calculate accuracies for Model A and Model B. \n",
    "\n",
    "We'll store all metrics associated with Model A and Model B in the `results_dict` below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN STARTER CODE\n",
    "\n",
    "results_dict = {\"A\": {}, \"B\": {}}\n",
    "\n",
    "### END STARTER CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict[\"A\"][\"accuracy\"] = None\n",
    "results_dict[\"B\"][\"accuracy\"] = None\n",
    "\n",
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert tests_lab1.ex1_2_1(\n",
    "    results_dict[\"A\"][\"accuracy\"]\n",
    "), \"Your answer is incorrect, see traceback above.\"\n",
    "print(\"Success\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert tests_lab1.ex1_2_2(\n",
    "    results_dict[\"B\"][\"accuracy\"]\n",
    "), \"Your answer is incorrect, see traceback above.\"\n",
    "print(\"Success\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(results_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Which model would you pick? \n",
    "rubric={reasoning:1}\n",
    "\n",
    "Which model would you pick simply based on the accuracy metric? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Precision, recall, f1-score\n",
    "rubric={autograde:6}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Calculate precision, recall, f1-score for Model A and Model B without using `scikit-learn` tools. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict[\"A\"][\"precision\"] = None\n",
    "results_dict[\"B\"][\"precision\"] = None\n",
    "results_dict[\"A\"][\"recall\"] = None\n",
    "results_dict[\"B\"][\"recall\"] = None\n",
    "results_dict[\"A\"][\"f1\"] = None\n",
    "results_dict[\"B\"][\"f1\"] = None\n",
    "\n",
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert tests_lab1.ex1_4_1(\n",
    "    results_dict[\"A\"][\"precision\"]\n",
    "), \"Your answer is incorrect, see traceback above.\"\n",
    "print(\"Success\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert tests_lab1.ex1_4_2(\n",
    "    results_dict[\"B\"][\"precision\"]\n",
    "), \"Your answer is incorrect, see traceback above.\"\n",
    "print(\"Success\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert tests_lab1.ex1_4_3(\n",
    "    results_dict[\"A\"][\"recall\"]\n",
    "), \"Your answer is incorrect, see traceback above.\"\n",
    "print(\"Success\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert tests_lab1.ex1_4_4(\n",
    "    results_dict[\"B\"][\"recall\"]\n",
    "), \"Your answer is incorrect, see traceback above.\"\n",
    "print(\"Success\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert tests_lab1.ex1_4_5(\n",
    "    results_dict[\"A\"][\"f1\"]\n",
    "), \"Your answer is incorrect, see traceback above.\"\n",
    "print(\"Success\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert tests_lab1.ex1_4_6(\n",
    "    results_dict[\"B\"][\"f1\"]\n",
    "), \"Your answer is incorrect, see traceback above.\"\n",
    "print(\"Success\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the dataframe with all results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(results_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Precision, recall, f1-score\n",
    "rubric={reasoning:2}\n",
    "\n",
    "**Your tasks:**\n",
    "1. Which metric is more informative in this case? Why? \n",
    "2. Which model would you pick based on this information? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (optional) 1.6 \n",
    "rubric={reasoning:1}\n",
    "\n",
    "**Your tasks:**\n",
    "1. Are confusion matrices, precision, and recall useful only where there is class imbalance? \n",
    "2. Provide 4 to 5 example classification datasets (with links) where accuracy metric would be misleading. Discuss which evaluation metric would be more appropriate for each dataset. You may consider datasets we used in 571. You could also look up datasets on Kaggle. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Classification evaluation metrics using `sklearn` <a name=\"2\"></a>\n",
    "<hr>\n",
    "\n",
    "In general, when the dataset is imbalanced, accuracy does not provide the whole story. In class, we looked at credit card fraud dataset which is a classic example of an imbalanced dataset. Another example is customer churn datasets, where most of the customers stay with the service and a small minority cancel their subscription. These are the people the company might want to target so in order to convince them to stay with them. For the next questions you will be using a [telecom customer churn dataset](https://www.kaggle.com/becksddf/churn-in-telecoms-dataset) from Kaggle. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the data CSV and save it under your lab folder. **Please do not push the CSV in your repo.** \n",
    "The starter code below reads the data CSV as a pandas dataframe and splits it into 70% train and 30% test. \n",
    "\n",
    "Note that `churn` column in the dataset is the target. \"True\" means the customer left the subscription (churned) and \"False\" means they stayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN STARTER CODE\n",
    "\n",
    "df = pd.read_csv(\"bigml_59c28831336c6604c800002a.csv\", encoding=\"latin-1\")\n",
    "train_df, test_df = train_test_split(df, test_size=0.3, random_state=123)\n",
    "train_df\n",
    "\n",
    "### END STARTER CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 EDA\n",
    "rubric={reasoning:3}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "Examine the distribution of target values in the train split. Do you see class imbalance? If yes, do we need to deal with it? Why or why not? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (optional) 2.2  \n",
    "rubric={reasoning:1}\n",
    "\n",
    "**Your tasks:**\n",
    "1. Choose 5 features which you think are most relevant for the prediction task. Examine correlations of these features with the target.  \n",
    "2. Which features look most promising? Note your observations.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Identify numeric, categorical, binary, and other features\n",
    "rubric={accuracy:1,reasoning:5}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Identify different feature types (e.g., numeric, categorical, binary, drop features). \n",
    "2. Separate `X` and `y` from `train_df` and `test_df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Feature transformations\n",
    "rubric={accuracy:4,reasoning:2}\n",
    "\n",
    "**Your tasks:**\n",
    "1. Describe your plan for feature transformations. Would you drop any features? \n",
    "2. Define a preprocessor with appropriate feature transformations using `ColumnTransformer`. \n",
    "\n",
    "Note that if you do not see any missing values, it's fine to skip imputation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 `DummyClassifier`\n",
    "rubric={accuracy:2, reasoning:1}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Carry out cross validation with `DummyClassifier`. Pass the following `scoring` metrics to `cross_validate`. \n",
    "    - accuracy\n",
    "    - f1\n",
    "    - recall\n",
    "    - precision\n",
    "    - average_precision\n",
    "    - roc_auc\n",
    "\n",
    "2. Comment on the scores given by `DummyClassifier`.  \n",
    "\n",
    "You may use the `results_dict` below to store all your results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Exploring different scoring methods on different classifiers\n",
    "rubric={accuracy:6,reasoning:4}\n",
    "\n",
    "In this exercise you will be using one of the most popular classifiers called [`RandomForestClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) which we haven't studied yet. We'll look at the details of the classifier later in the course but that shouldn't prevent you from using it. At this point you should feel comfortable using models with our usual ML workflow even if you haven't seen them before. \n",
    "\n",
    "**Your tasks:**\n",
    "1. For each of the following classifiers in the starter code below, carry out cross-validation using the `cross_validate` function and the following evaluation metrics. Note that you can pass multiple [scoring metrics](https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter) as a list or a dict to the `scoring` parameter. \n",
    "    - `accuracy`\n",
    "    - `precision`\n",
    "    - `recall`\n",
    "    - `f1`\n",
    "    - `average_precision`\n",
    "    - `roc_auc`\n",
    "2. Discuss the results focusing on the following points\n",
    "    1. How do the results compare to the baseline, i.e., `DummyClassifier`?\n",
    "    2. Comment on precision and recall of `LogisticRegression`.    \n",
    "    3. Comment on precision and recall of `RandomForestClassifier`.    \n",
    "    4. How are the results affected when `class_weight` parameter is used?\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN STARTER CODE\n",
    "\n",
    "classifiers = {\n",
    "    \"Logistic Regression\": LogisticRegression(),\n",
    "    \"Logistic Regression (balanced)\": LogisticRegression(class_weight=\"balanced\"),\n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "    \"Random Forest (balanced)\": RandomForestClassifier(class_weight=\"balanced\"),\n",
    "}\n",
    "\n",
    "### END STARTER CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 Hyperparameter optimization and test results\n",
    "rubric={accuracy:4,viz:2,reasoning:4}\n",
    "\n",
    "The starter code below defines a pipeline with a preprocessor (assuming that your preprocessor is named as `preprocessor`), a `RandomForestClassifier` model, and a parameter grid. \n",
    "\n",
    "**Your taks:**\n",
    "1. Carry out hyperparameter optimization with `RandomizedSearchCV` using this pipeline and parameter distributions using `f1` score for optimization. \n",
    "2. Try the best model on the test set. \n",
    "    - Display confusion matrix. \n",
    "    - Display classification report. \n",
    "    - Show precision-recall curve with average precision score.     \n",
    "    - Show ROC curve with AUC. \n",
    "3. Comment on the results.     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN STARTER CODE\n",
    "\n",
    "import scipy\n",
    "from scipy.stats import randint\n",
    "\n",
    "rf_pipeline = make_pipeline(\n",
    "    preprocessor, RandomForestClassifier(class_weight=\"balanced\", random_state=123)\n",
    ")\n",
    "\n",
    "param_dist = {\n",
    "    \"randomforestclassifier__n_estimators\": scipy.stats.randint(low=10, high=300),\n",
    "    \"randomforestclassifier__max_depth\": scipy.stats.randint(low=2, high=20),\n",
    "}\n",
    "\n",
    "### END STARTER CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (optional) 2.8 SMOTE\n",
    "rubric={reasoning:1}\n",
    "\n",
    "**Your tasks:**\n",
    "- Instead of using `class_weight`, use SMOTE with the above classifiers to deal with class imbalance. \n",
    "- Compare your results with the results when you used `class_weight`?\n",
    "\n",
    "Note that you'll have to use [imbalanced-learn](https://imbalanced-learn.readthedocs.io/en/stable/install.html) for SMOTE. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Regression metrics <a name=\"3\"></a>\n",
    "<hr> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this exercise, we'll use [California housing dataset](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_california_housing.html) from `sklearn datasets`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN STARTER CODE\n",
    "\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "housing_df = fetch_california_housing(as_frame=True).frame\n",
    "\n",
    "### END STARTER CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.1: Data spitting and exploration \n",
    "rubric={accuracy:1,reasoning:3}\n",
    "\n",
    "**Your tasks**\n",
    "\n",
    "1. Split the data into train and test portions. \n",
    "2. Explore the train split. Do you need to apply any transformations? If yes, which transformations would you apply? \n",
    "3. Separate `X` and `y` in train and test splits. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Baseline: DummyRegressor \n",
    "rubric={accuracy:1,reasoning:1}\n",
    "\n",
    "**Your tasks:**\n",
    "1. Carry out cross-validation using `DummyRegressor`. \n",
    "2. What metric is used by default? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Different regressors\n",
    "rubric={accuracy:4,reasoning:2}\n",
    "\n",
    "In this exercise, we are going to use `RandomForestRegressor` model which we haven't looked into yet. As I said before, at this point you should feel comfortable using models with our usual ML workflow even if you don't know the details. We'll talk about `RandomForestRegressor` later in the course.  \n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Using the models in the starter code below, carry out cross validation with each model (use a pipeline with the model as an estimator if you are applying any transformations) with all of the following evaluation metrics. You can do it by passing the evaluation metrics to `scoring` argument of `cross_validate`. \n",
    "    - neg_mean_squared_error\n",
    "    - neg_root_mean_squared_error\n",
    "    - neg_mean_absolute_error\n",
    "    - r2\n",
    "    - mape_scorer (user-defined scorer given in the starter code below)\n",
    "2. Show results as a dataframe with models as columns and rows with different metrics. \n",
    "3. Interpret the results. How do the models compare to the baseline? Which model seems to be performing well with all different metrics? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN STARTER CODE\n",
    "\n",
    "\n",
    "def mape(true, pred):\n",
    "    return 100.0 * np.mean(np.abs((pred - true) / true))\n",
    "\n",
    "\n",
    "# make a scorer function that we can pass into cross-validation\n",
    "mape_scorer = make_scorer(mape, greater_is_better=False)\n",
    "\n",
    "models = {\n",
    "    \"Ridge\": Ridge(),\n",
    "    \"Random Forest\": RandomForestRegressor(),\n",
    "}\n",
    "\n",
    "### END STARTER CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Hyperparameter optimization \n",
    "rubric={accuracy:2,reasoning:3}\n",
    "\n",
    "**Your tasks:**\n",
    "1. Carry out hyperparameter optimization using `RandomizedSearchCV` and `Ridge`. The `alpha` hyperparameter of `Ridge` controls the fundamental tradeoff. Choose the metric of your choice for hyperparameter optimization. \n",
    "2. Are you getting better scores compared to the default values? \n",
    "3. Try the best `Ridge` model on the test set.\n",
    "4. Comment on the results.  \n",
    "\n",
    "**Note: If you get errors with `n_jobs=-1`, try to use `n_jobs=1`, i.e., sequential processing.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (optional) 3.5 Hyperparameter optimization with `RandomForestRegressor`\n",
    "rubric={reasoning:1}\n",
    "\n",
    "**Your tasks:**\n",
    "1. Carry out hyperparameter optimization using `RandomForestRegressor` and hyperparameter optimization tool of your choice. You can find the relevant hyperparameters in the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html). \n",
    "2. Are you getting better scores compared to the default values of the regressor? \n",
    "3. Try the best model on the test set and comment on the results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Model interpretation  \n",
    "rubric={viz:2,reasoning:2}\n",
    "\n",
    "Ridge is a linear model and it learns coefficients associated with each feature during fit. \n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Visualize coefficients learned by the `best_estimator_` of random search associated with each feature as a pandas dataframe with two columns: features and coefficients. \n",
    "2. Increasing which feature values would result in higher and lower housing price? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission to Canvas\n",
    "\n",
    "**PLEASE READ: When you are ready to submit your assignment do the following:**\n",
    "\n",
    "- Run all cells in your notebook to make sure there are no errors by doing Kernel -->  Restart Kernel and Run All Cells...\n",
    "- If you are using the \"573\" `conda` environment, make sure to select it before running all cells. \n",
    "- Convert your notebook to .html format using the `convert_notebook()` function below or by File -> Export Notebook As... -> Export Notebook to HTML\n",
    "- Run the code `submit()` below to go through an interactive submission process to Canvas.\n",
    "After submission, be sure to do a final push of all your work to GitHub (including the rendered html file)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from canvasutils.submit import convert_notebook, submit\n",
    "\n",
    "# convert_notebook(\"lab1.ipynb\", \"html\")  # uncomment and run when you want to try convert your notebook (or you can convert manually from the File menu)\n",
    "# submit(course_code=53670, token=False)  # uncomment and run when ready to submit to Canvas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations on finishing the lab!! "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:571]",
   "language": "python",
   "name": "conda-env-571-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "name": "_merged",
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "438px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
