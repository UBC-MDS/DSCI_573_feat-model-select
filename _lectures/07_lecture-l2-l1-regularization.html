

<!DOCTYPE html>


<html lang="en" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>DSCI 573: Feature and Model Selection &#8212; DSCI 573 Feature and Model Selection</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=e353d410970836974a52" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=e353d410970836974a52" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=e353d410970836974a52" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52" />

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = '_lectures/07_lecture-l2-l1-regularization';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  

<a class="navbar-brand logo" href="../README.html">
  
  
  
  
    
    
      
    
    
    <img src="../_static/mds-hex-sticker.png" class="logo__image only-light" alt="Logo image"/>
    <script>document.write(`<img src="../_static/mds-hex-sticker.png" class="logo__image only-dark" alt="Logo image"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Lectures</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../lectures/01_classification-metrics.html">Lecture 1: Classification metrics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../lectures/02_regression-metrics.html">Lecture 2: Regression metrics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../lectures/03_feature-engineering.html">Lecture 3: Feature engineering</a></li>
<li class="toctree-l1"><a class="reference internal" href="../lectures/04_feat-importances-selection.html">Lecture 4: Feature importances and feature selection</a></li>
<li class="toctree-l1"><a class="reference internal" href="../lectures/05_loss-functions-regularization_intro.html">Lecture 5: Loss functions, intro to regularization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../lectures/06_L2-L1-regularization.html">Lecture 6: L2- and L1-Regularization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../lectures/07_ensembles.html">Lecture 7: Ensembles</a></li>
<li class="toctree-l1"><a class="reference internal" href="../lectures/08_model-transparency-conclusion.html">Lecture 8: Model transparency and conclusion</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Attribution</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../attribution.html">Attributions</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/_lectures/07_lecture-l2-l1-regularization.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>


<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>DSCI 573: Feature and Model Selection</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#lecture-7-regularization">Lecture 7: Regularization</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-outcomes">Learning outcomes</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#motivation">1. Motivation</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#complex-models-and-the-fundamental-tradeoff">Complex models and the fundamental tradeoff</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#controlling-model-complexity">Controlling model complexity</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#idea-of-regularization-pick-the-line-hyperplane-with-smaller-slope">Idea of regularization: Pick the line/hyperplane with smaller slope</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#why-pick-the-line-hyperplane-with-smaller-slope">Why pick the line/hyperplane with smaller slope?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#regularization-l0-penalty-you-have-seen-before">Regularization: L0 penalty you have seen before</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Regularization: L0 penalty you have seen before</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#reminder-l0-l1-and-l2-norms">Reminder: L0, L1, and L2 norms</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#terminology-and-notation-l0-l1-and-l2-norms">Terminology and notation: L0, L1, and L2 norms</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#l2-regularization">2. L2 regularization</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#l2-penalty">L2 penalty</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">L2 regularization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#have-we-seen-this-before">Have we seen this before?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#size-of-regression-weights-and-overfitting">Size of regression weights and overfitting</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">Size of regression weights and overfitting</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#why-are-small-weights-better">Why are small weights better?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">Why are small weights better?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#how-alpha-affects-the-weights">How <code class="docutils literal notranslate"><span class="pre">alpha</span></code> affects the weights</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#l2-regularization-shrinking-example">L2-Regularization “Shrinking” Example</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#alpha-values-and-fit-of-the-model"><code class="docutils literal notranslate"><span class="pre">alpha</span></code> values and fit of the model</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-weights-become-smaller-but-never-become-zero">The weights become smaller but never become zero</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#regularization-path">Regularization path</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#question-l0-versus-l2-regularization-solution">Question: L0 versus L2 regularization (solution)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#how-to-pick-lambda">How to pick <span class="math notranslate nohighlight">\(\lambda\)</span>?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#should-we-regularize-the-y-intercept">Should we regularize the y-intercept?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#some-properties-of-l2-regularization">Some properties of L2 regularization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#interim-summary-l2-regularization">Interim summary: L2 regularization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#l1-regularization">3. L1 regularization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id5">L1-regularization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#similarities-with-l2-regularization">Similarities with L2-regularization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#terminology-and-notation-ridge-and-lasso">Terminology and notation: Ridge and Lasso</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-weights-become-smaller-and-smaller-and-become-zero">The weights become smaller and smaller and become zero</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#terminology-and-notation-sparsity">Terminology and notation: Sparsity</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#similarities-between-l1-and-l0-regularization">Similarities between L1- and L0-regularization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-l0-vs-l1-vs-l2">Example: L0 vs. L1 vs. L2</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#which-one-would-you-choose-with-l0-regularization">Which one would you choose with L0 regularization?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#which-one-would-you-choose-with-l1-regularization">Which one would you choose with L1 regularization?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#which-one-would-you-choose-with-l2-regularization">Which one would you choose with L2 regularization?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#optional-sparsity-and-regularization">(Optional) Sparsity and Regularization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#optional-sparsity-and-regularization-with-d-1">(optional) Sparsity and Regularization (with d=1)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#some-properties-of-l1-regularization">Some properties of L1 regularization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#l1-for-feature-selection">L1 for feature selection</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#feature-selection-using-l1-regularization">4. Feature selection using L1 regularization</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#feature-selection-methods-so-far">Feature selection methods so far</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#our-usual-game-of-getting-feature-names-for-the-coefficients">Our usual game of getting feature names for the coefficients</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#regularized-logistic-regression">Regularized logistic regression</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#logistic-regression-with-regularization">Logistic regression with regularization</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#regularization-scaling-and-colinearity">5. Regularization: scaling and colinearity</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#regularization-and-scaling">Regularization and scaling</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#scaling-features">Scaling features</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#normalizing-linearregression">Normalizing LinearRegression</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#scaling-summary">Scaling: summary</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#collinearity-and-regularization">Collinearity and regularization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#elastic-nets">Elastic nets</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#how-to-use-regularization-with-scikit-learn-some-examples">How to use regularization with <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code>: some examples</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#summary-l-regularization">Summary: L* regularization</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id6">Summary: L* regularization</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#true-false-questions-l2-regularization">True/False questions: L2-regularization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#true-false-questions-l1-regularization">True/False questions: L1-regularization</a></li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="dsci-573-feature-and-model-selection">
<h1>DSCI 573: Feature and Model Selection<a class="headerlink" href="#dsci-573-feature-and-model-selection" title="Permalink to this heading">#</a></h1>
<section id="lecture-7-regularization">
<h2>Lecture 7: Regularization<a class="headerlink" href="#lecture-7-regularization" title="Permalink to this heading">#</a></h2>
<p>UBC Master of Data Science program, 2020-21</p>
<p>Instructor: Varada Kolhatkar</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>

<span class="o">%</span><span class="k">matplotlib</span> inline
<span class="kn">import</span> <span class="nn">string</span>
<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">deque</span>

<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">numpy.random</span> <span class="k">as</span> <span class="nn">npr</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="c1"># data</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span>
<span class="kn">from</span> <span class="nn">sklearn.compose</span> <span class="kn">import</span> <span class="n">ColumnTransformer</span><span class="p">,</span> <span class="n">make_column_transformer</span>
<span class="kn">from</span> <span class="nn">sklearn.dummy</span> <span class="kn">import</span> <span class="n">DummyClassifier</span><span class="p">,</span> <span class="n">DummyRegressor</span>
<span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestClassifier</span><span class="p">,</span> <span class="n">RandomForestRegressor</span>
<span class="kn">from</span> <span class="nn">sklearn.feature_extraction.text</span> <span class="kn">import</span> <span class="n">CountVectorizer</span>

<span class="c1"># Feature selection</span>
<span class="kn">from</span> <span class="nn">sklearn.feature_selection</span> <span class="kn">import</span> <span class="n">RFE</span><span class="p">,</span> <span class="n">RFECV</span><span class="p">,</span> <span class="n">SelectFromModel</span>
<span class="kn">from</span> <span class="nn">sklearn.impute</span> <span class="kn">import</span> <span class="n">SimpleImputer</span>

<span class="c1"># classifiers / models</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">Lasso</span><span class="p">,</span>
    <span class="n">LassoCV</span><span class="p">,</span>
    <span class="n">LinearRegression</span><span class="p">,</span>
    <span class="n">LogisticRegression</span><span class="p">,</span>
    <span class="n">Ridge</span><span class="p">,</span>
    <span class="n">RidgeCV</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># other</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span><span class="p">,</span> <span class="n">log_loss</span><span class="p">,</span> <span class="n">make_scorer</span><span class="p">,</span> <span class="n">mean_squared_error</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">GridSearchCV</span><span class="p">,</span>
    <span class="n">RandomizedSearchCV</span><span class="p">,</span>
    <span class="n">ShuffleSplit</span><span class="p">,</span>
    <span class="n">cross_val_score</span><span class="p">,</span>
    <span class="n">cross_validate</span><span class="p">,</span>
    <span class="n">train_test_split</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">sklearn.pipeline</span> <span class="kn">import</span> <span class="n">Pipeline</span><span class="p">,</span> <span class="n">make_pipeline</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">OneHotEncoder</span><span class="p">,</span>
    <span class="n">OrdinalEncoder</span><span class="p">,</span>
    <span class="n">PolynomialFeatures</span><span class="p">,</span>
    <span class="n">StandardScaler</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="kn">import</span> <span class="n">SVC</span><span class="p">,</span> <span class="n">SVR</span>
<span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeClassifier</span>

<span class="n">pd</span><span class="o">.</span><span class="n">set_option</span><span class="p">(</span><span class="s2">&quot;display.max_colwidth&quot;</span><span class="p">,</span> <span class="mi">200</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<section id="learning-outcomes">
<h3>Learning outcomes<a class="headerlink" href="#learning-outcomes" title="Permalink to this heading">#</a></h3>
<p>From this lecture, students are expected to be able to:</p>
<ul class="simple">
<li><p>Explain the general idea of L2 regularization.</p></li>
<li><p>Explain the relation between the size of regression weights and overfitting.</p></li>
<li><p>Use L2 regularization (Ridge) using <code class="docutils literal notranslate"><span class="pre">sklearn</span></code>.</p></li>
<li><p>Explain the general idea of L1-regularization.</p></li>
<li><p>Use L1-regularization (Lasso) using <code class="docutils literal notranslate"><span class="pre">sklearn</span></code>.</p></li>
<li><p>Discuss sparsity in L1-regularization.</p></li>
<li><p>Compare L0-, L1-, and L2-regularization.</p></li>
<li><p>Use L1 regularization for feature selection.</p></li>
<li><p>Explain the importance of scaling when using L1- and L2-regularization</p></li>
<li><p>Briefly explain how L1 and L2 regularization behave in the presence of collinear features.</p></li>
</ul>
</section>
</section>
<section id="motivation">
<h2>1. Motivation<a class="headerlink" href="#motivation" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p>Video 1</p></li>
</ul>
<section id="complex-models-and-the-fundamental-tradeoff">
<h3>Complex models and the fundamental tradeoff<a class="headerlink" href="#complex-models-and-the-fundamental-tradeoff" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>We’ve said that complex models tend to overfit more.</p></li>
<li><p>Recall: polynomial degree and train vs. validation scores.</p></li>
</ul>
<p>Let’s generate some synthetic data</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">n</span> <span class="o">=</span> <span class="mi">20</span>
<span class="n">X_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>
<span class="n">y_train</span> <span class="o">=</span> <span class="n">X_train</span> <span class="o">**</span> <span class="mi">3</span> <span class="o">+</span> <span class="n">npr</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n</span><span class="p">)</span> <span class="o">*</span> <span class="mi">3</span>

<span class="n">n</span> <span class="o">=</span> <span class="mi">20</span>
<span class="n">X_valid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>
<span class="n">y_valid</span> <span class="o">=</span> <span class="n">X_valid</span> <span class="o">**</span> <span class="mi">3</span> <span class="o">+</span> <span class="n">npr</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n</span><span class="p">)</span> <span class="o">*</span> <span class="mi">3</span>

<span class="c1"># transforming the data to include another axis</span>
<span class="n">X_train</span> <span class="o">=</span> <span class="n">X_train</span><span class="p">[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]</span>
<span class="n">y_train</span> <span class="o">=</span> <span class="n">y_train</span><span class="p">[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]</span>

<span class="n">X_valid</span> <span class="o">=</span> <span class="n">X_valid</span><span class="p">[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]</span>
<span class="n">y_valid</span> <span class="o">=</span> <span class="n">y_valid</span><span class="p">[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]</span>

<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;blue&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;matplotlib.collections.PathCollection at 0x7fea2acad2e0&gt;
</pre></div>
</div>
<img alt="../_images/1869b3ae1e2526554685cd1a0f0cdfcaa457fc3d7996f6c1a80ddbf64d515b25.png" src="../_images/1869b3ae1e2526554685cd1a0f0cdfcaa457fc3d7996f6c1a80ddbf64d515b25.png" />
</div>
</div>
<p>Let’s fit polynomials of different degrees on the synthetic data.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">plot_train_poly_deg</span><span class="p">():</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
    <span class="n">count</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">degrees</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">18</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">deg</span> <span class="ow">in</span> <span class="n">degrees</span><span class="p">:</span>  <span class="c1"># needs scikit-learn-0.20</span>
        <span class="n">pipe_poly_lr</span> <span class="o">=</span> <span class="n">make_pipeline</span><span class="p">(</span><span class="n">PolynomialFeatures</span><span class="p">(</span><span class="n">degree</span><span class="o">=</span><span class="n">deg</span><span class="p">),</span> <span class="n">LinearRegression</span><span class="p">())</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">count</span><span class="p">)</span>
        <span class="n">pipe_poly_lr</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;blue&quot;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">pipe_poly_lr</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train</span><span class="p">),</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;green&quot;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span>
            <span class="s2">&quot;p = </span><span class="si">%s</span><span class="s2">, train = </span><span class="si">%0.3f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">deg</span><span class="p">),</span> <span class="n">pipe_poly_lr</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">))</span>
        <span class="p">)</span>
        <span class="n">count</span> <span class="o">+=</span> <span class="mi">1</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plot_train_poly_deg</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/cd24d568729df94e87396736850bea12be85dd4d34e45c805efffd9c20ddfbfc.png" src="../_images/cd24d568729df94e87396736850bea12be85dd4d34e45c805efffd9c20ddfbfc.png" />
</div>
</div>
<p>The training score goes up as we increase the degree of the polynomial, and is highest for <span class="math notranslate nohighlight">\(p = 18\)</span>.</p>
<ul class="simple">
<li><p>How good are these models on the validation set?</p></li>
<li><p>The validation score is highest for <span class="math notranslate nohighlight">\(p = 3\)</span>, and it goes down as we increase the degree after that; we start overfitting after <span class="math notranslate nohighlight">\(p=3\)</span>.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">plot_valid_poly_deg</span><span class="p">():</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
    <span class="n">count</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">degrees</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">18</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">deg</span> <span class="ow">in</span> <span class="n">degrees</span><span class="p">:</span>  <span class="c1"># needs scikit-learn-0.20</span>
        <span class="n">pipe_poly_lr</span> <span class="o">=</span> <span class="n">make_pipeline</span><span class="p">(</span><span class="n">PolynomialFeatures</span><span class="p">(</span><span class="n">degree</span><span class="o">=</span><span class="n">deg</span><span class="p">),</span> <span class="n">LinearRegression</span><span class="p">())</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">count</span><span class="p">)</span>
        <span class="n">pipe_poly_lr</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_valid</span><span class="p">,</span> <span class="n">y_valid</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;blue&quot;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">pipe_poly_lr</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train</span><span class="p">),</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;green&quot;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
        <span class="n">count</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span>
            <span class="s2">&quot;p = </span><span class="si">%s</span><span class="s2">, valid = </span><span class="si">%0.3f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">deg</span><span class="p">),</span> <span class="n">pipe_poly_lr</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_valid</span><span class="p">,</span> <span class="n">y_valid</span><span class="p">))</span>
        <span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plot_valid_poly_deg</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/f3ebbb7d4b3e9415b9b46ac05a51eab12c5e9d87ef7c5be805b90f2b8d0a6ee6.png" src="../_images/f3ebbb7d4b3e9415b9b46ac05a51eab12c5e9d87ef7c5be805b90f2b8d0a6ee6.png" />
</div>
</div>
<ul class="simple">
<li><p>So there is a tradeoff between complexity of models and the validation score.</p></li>
<li><p>But what if we need complex models?</p></li>
<li><p>In supervised ML we try to find the mapping between <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(y\)</span>, and usually the “true” mapping from <span class="math notranslate nohighlight">\(X\)</span> to <span class="math notranslate nohighlight">\(y\)</span> is complex.</p>
<ul>
<li><p>Might need high-degree polynomial.</p></li>
<li><p>Might need to use many features, and don’t know “relevant” ones.</p></li>
</ul>
</li>
</ul>
</section>
<section id="controlling-model-complexity">
<h3>Controlling model complexity<a class="headerlink" href="#controlling-model-complexity" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>Two methods to control complexity:</p>
<ul>
<li><p>Model averaging: average over multiple models to decrease variance (e.g., random forests). <img alt="Screen%20Shot%202020-11-30%20at%209.36.10%20AM.png" src="_lectures/attachment:Screen%20Shot%202020-11-30%20at%209.36.10%20AM.png" /></p></li>
<li><p><strong>Regularization: add a penalty on the complexity of the model</strong></p>
<ul>
<li><p>This lecture!</p></li>
</ul>
</li>
</ul>
</li>
</ul>
</section>
<section id="idea-of-regularization-pick-the-line-hyperplane-with-smaller-slope">
<h3>Idea of regularization: Pick the line/hyperplane with smaller slope<a class="headerlink" href="#idea-of-regularization-pick-the-line-hyperplane-with-smaller-slope" title="Permalink to this heading">#</a></h3>
<br>
<center>
<img src='imgs/green_or_red.png' width="600" height="600" />
</center>    
<ul class="simple">
<li><p>Assuming red and green models have the same training score and if you are forced to choose one of them, which one would you pick?</p>
<ul>
<li><p><strong>Pick green because its slope is smaller.</strong></p></li>
</ul>
</li>
</ul>
</section>
<section id="why-pick-the-line-hyperplane-with-smaller-slope">
<h3>Why pick the line/hyperplane with smaller slope?<a class="headerlink" href="#why-pick-the-line-hyperplane-with-smaller-slope" title="Permalink to this heading">#</a></h3>
<br>
<center>
<img src='imgs/green_or_red.png' width="500" height="500" />
</center>    
<ul class="simple">
<li><p>Small change in <span class="math notranslate nohighlight">\(x_i\)</span> has a smaller change in prediction <span class="math notranslate nohighlight">\(y_i\)</span></p></li>
<li><p>Green line’s predictions are less sensitive to the training data.</p></li>
<li><p>Since green <span class="math notranslate nohighlight">\(w\)</span> is less sensitive to training data, validation error might be lower.</p></li>
</ul>
<p>In this lecture we are going to explore methods to make <span class="math notranslate nohighlight">\(w\)</span> less sensitive to the data.</p>
</section>
<section id="regularization-l0-penalty-you-have-seen-before">
<h3>Regularization: L0 penalty you have seen before<a class="headerlink" href="#regularization-l0-penalty-you-have-seen-before" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>Least squares loss before applying penalty:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}f(w) = \frac{1}{2}\sum_i^n(w^TX_i - y_i)^2 \text{ or }\\
f(x) = \frac{1}{2}\lVert{Xw -y}\rVert^2_2\end{split}\]</div>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\lVert{Xw -y}\rVert^2_2 \rightarrow\)</span> square of the L2 norm <span class="math notranslate nohighlight">\(Xw -y\)</span></p></li>
</ul>
</section>
<section id="id1">
<h3>Regularization: L0 penalty you have seen before<a class="headerlink" href="#id1" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>Least squares loss after applying L0 penalty:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[f(w) = \frac{1}{2}\lVert{Xw -y}\rVert^2_2 + \lambda \lVert w\rVert_0\]</div>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\lVert{Xw -y}\rVert^2_2 \rightarrow\)</span> square of the L2 norm <span class="math notranslate nohighlight">\(Xw -y\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\lambda \rightarrow\)</span> penalty parameter</p></li>
<li><p><span class="math notranslate nohighlight">\(\lVert w\rVert_0 \rightarrow\)</span>  L0 norm of <span class="math notranslate nohighlight">\(w\)</span></p>
<ul>
<li><p>The number of non-zero values in <span class="math notranslate nohighlight">\(w\)</span>.</p></li>
</ul>
</li>
</ul>
</section>
<section id="reminder-l0-l1-and-l2-norms">
<h3>Reminder: L0, L1, and L2 norms<a class="headerlink" href="#reminder-l0-l1-and-l2-norms" title="Permalink to this heading">#</a></h3>
</section>
<section id="terminology-and-notation-l0-l1-and-l2-norms">
<h3>Terminology and notation: L0, L1, and L2 norms<a class="headerlink" href="#terminology-and-notation-l0-l1-and-l2-norms" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>L0 “norm” <span class="math notranslate nohighlight">\(\lVert w \rVert \rightarrow\)</span> the number of non-zero elements in <span class="math notranslate nohighlight">\(w\)</span></p></li>
<li><p>L1 norm <span class="math notranslate nohighlight">\(\lVert w \rVert_1 = \lvert w_1 \rvert + \lvert w_2 \rvert + \dots + \lvert w_n \rvert\)</span></p></li>
<li><p>L2 norm <span class="math notranslate nohighlight">\(\lVert w \rVert_2 = (w_1^2 + w_2^2 + \dots + w_n^2)^{1/2}\)</span></p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">numpy</span> <span class="kn">import</span> <span class="n">array</span>
<span class="kn">from</span> <span class="nn">numpy.linalg</span> <span class="kn">import</span> <span class="n">norm</span>

<span class="n">w</span> <span class="o">=</span> <span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>

<span class="n">l0</span> <span class="o">=</span> <span class="n">norm</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>  <span class="c1"># number of non-zero values</span>
<span class="n">l1</span> <span class="o">=</span> <span class="n">norm</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>  <span class="c1"># sum of absolute values</span>
<span class="n">l2</span> <span class="o">=</span> <span class="n">norm</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>  <span class="c1"># square root of sum of the squared values</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;The l0 norm of </span><span class="si">%s</span><span class="s2"> is: </span><span class="si">%0.3f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">l0</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;The l1 norm of </span><span class="si">%s</span><span class="s2"> is: </span><span class="si">%0.3f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">l1</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;The l2 norm of </span><span class="si">%s</span><span class="s2"> is: </span><span class="si">%0.3f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">l2</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>The l0 norm of [0 2 4] is: 2.000
The l1 norm of [0 2 4] is: 6.000
The l2 norm of [0 2 4] is: 4.472
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># norms of a vector</span>
<span class="kn">from</span> <span class="nn">numpy</span> <span class="kn">import</span> <span class="n">array</span>
<span class="kn">from</span> <span class="nn">numpy.linalg</span> <span class="kn">import</span> <span class="n">norm</span>

<span class="n">w</span> <span class="o">=</span> <span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>
<span class="c1"># l0 norm is the number of non-zero values in a vector</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;The l0 norm of </span><span class="si">%s</span><span class="s2"> is: </span><span class="si">%0.3f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">norm</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="mi">0</span><span class="p">)))</span>

<span class="c1"># l1 norm is the sum of the absolute values in a vector.</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;The l1 norm of </span><span class="si">%s</span><span class="s2"> is: </span><span class="si">%0.3f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">norm</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>

<span class="c1"># l2 norm is square root of the sum of the squared values in a vector.</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;The l2 norm of </span><span class="si">%s</span><span class="s2"> is: </span><span class="si">%0.3f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">norm</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="mi">2</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>The l0 norm of [ 1 -2  3  0] is: 3.000
The l1 norm of [ 1 -2  3  0] is: 6.000
The l2 norm of [ 1 -2  3  0] is: 3.742
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="l2-regularization">
<h2>2. L2 regularization<a class="headerlink" href="#l2-regularization" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p>Video 2</p></li>
</ul>
<section id="l2-penalty">
<h3>L2 penalty<a class="headerlink" href="#l2-penalty" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>Standard regularization strategy is L2 regularization</p>
<ul>
<li><p>We incorporate L2 penalty in the loss function <span class="math notranslate nohighlight">\(f(w)\)</span>:
$<span class="math notranslate nohighlight">\(f(w) = \frac{1}{2}\sum_i^n(w^TX_i - y_i)^2 + \frac{\lambda}{2}\sum_j^d w_j^2 \text{ or }\)</span><span class="math notranslate nohighlight">\( 
\)</span><span class="math notranslate nohighlight">\(f(w) = \frac{1}{2}\lVert Xw - y\rVert_2^2 + \frac{\lambda}{2} \lVert w\rVert_2^2\)</span>$</p></li>
</ul>
</li>
<li><p><span class="math notranslate nohighlight">\(\lVert Xw - y\rVert_2^2 \rightarrow\)</span> square of the <span class="math notranslate nohighlight">\(L2\)</span> norm of <span class="math notranslate nohighlight">\(Xw -y\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\lambda \rightarrow\)</span> regularization strength</p></li>
<li><p><span class="math notranslate nohighlight">\(\lVert w\rVert_2^2 \rightarrow\)</span>  square of the L2 norm of <span class="math notranslate nohighlight">\(w\)</span></p>
<ul>
<li><p>sum of the squared weight values.</p></li>
</ul>
</li>
</ul>
</section>
<section id="id2">
<h3>L2 regularization<a class="headerlink" href="#id2" title="Permalink to this heading">#</a></h3>
<div class="math notranslate nohighlight">
\[f(w) = \frac{1}{2}\lVert Xw - y\rVert_2^2 + \frac{\lambda}{2} \lVert w\rVert_2^2\]</div>
<ul class="simple">
<li><p>Objective balances getting low error vs. having small slopes <span class="math notranslate nohighlight">\(w_j\)</span></p></li>
<li><p>In terms of fundamental trade-off:</p>
<ul>
<li><p>You can increase the training error.</p></li>
<li><p>Nearly-always reduces overfitting and the validation error.</p></li>
</ul>
</li>
</ul>
</section>
<section id="have-we-seen-this-before">
<h3>Have we seen this before?<a class="headerlink" href="#have-we-seen-this-before" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p><a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html">Ridge</a>: Linear Regression with L2 regularization</p></li>
</ul>
<blockquote>
<div><p>class sklearn.linear_model.Ridge(alpha=1.0, fit_intercept=True, normalize=False, copy_X=True, max_iter=None, tol=0.001, solver=’auto’, random_state=None)</p>
</div></blockquote>
<blockquote>
<div><p>Linear least squares with l2 regularization. Minimizes the objective function: ||y - Xw||^2_2 + alpha * ||w||^2_2
This model solves a regression model where the loss function is the linear least squares function and regularization is given by the l2-norm. Also known as Ridge Regression or Tikhonov regularization.</p>
</div></blockquote>
<ul class="simple">
<li><p>Uses the hyperparameter <span class="math notranslate nohighlight">\(\alpha\)</span> for regularization strength instead of <span class="math notranslate nohighlight">\(\lambda\)</span>; larger value of <span class="math notranslate nohighlight">\(\alpha\)</span> means more regularization.
$<span class="math notranslate nohighlight">\(f(w) = \lVert Xw - y\rVert_2^2 + \alpha \lVert w\rVert_2^2\)</span>$</p></li>
</ul>
<ul class="simple">
<li><p><a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html">LogisticRegression</a>: Logistic Regression with L2 regularization</p></li>
</ul>
<blockquote>
<div><p>class sklearn.linear_model.LogisticRegression(penalty=’l2’, *, dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None, solver=’lbfgs’, max_iter=100, multi_class=’auto’, verbose=0, warm_start=False, n_jobs=None, l1_ratio=None)</p>
</div></blockquote>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">C</span></code>: default=1.0
Inverse of regularization strength; must be a positive float. Like in support vector machines, smaller values specify stronger regularization.</p></li>
</ul>
</section>
<section id="size-of-regression-weights-and-overfitting">
<h3>Size of regression weights and overfitting<a class="headerlink" href="#size-of-regression-weights-and-overfitting" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>Bigger values of weights means the model is very sensitive to the training data.</p></li>
<li><p>Let’s use Ridge with no regularization on polynomials with different degrees.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">alpha</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">degrees</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="mi">20</span><span class="p">]</span>
<span class="n">data</span> <span class="o">=</span> <span class="p">{}</span>
<span class="k">for</span> <span class="n">deg</span> <span class="ow">in</span> <span class="n">degrees</span><span class="p">:</span>
    <span class="n">pipe_poly_ridge</span> <span class="o">=</span> <span class="n">make_pipeline</span><span class="p">(</span>
        <span class="n">PolynomialFeatures</span><span class="p">(</span><span class="n">degree</span><span class="o">=</span><span class="n">deg</span><span class="p">,</span> <span class="n">include_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span> <span class="n">Ridge</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">)</span>
    <span class="p">)</span>
    <span class="n">pipe_poly_ridge</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

    <span class="n">data</span><span class="o">.</span><span class="n">setdefault</span><span class="p">(</span><span class="s2">&quot;degree&quot;</span><span class="p">,</span> <span class="p">[])</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">deg</span><span class="p">))</span>
    <span class="n">data</span><span class="o">.</span><span class="n">setdefault</span><span class="p">(</span><span class="s2">&quot;train_score&quot;</span><span class="p">,</span> <span class="p">[])</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
        <span class="nb">str</span><span class="p">(</span><span class="nb">round</span><span class="p">(</span><span class="n">pipe_poly_ridge</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">),</span> <span class="mi">3</span><span class="p">))</span>
    <span class="p">)</span>
    <span class="n">data</span><span class="o">.</span><span class="n">setdefault</span><span class="p">(</span><span class="s2">&quot;valid_score&quot;</span><span class="p">,</span> <span class="p">[])</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
        <span class="nb">str</span><span class="p">(</span><span class="nb">round</span><span class="p">(</span><span class="n">pipe_poly_ridge</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_valid</span><span class="p">,</span> <span class="n">y_valid</span><span class="p">),</span> <span class="mi">3</span><span class="p">))</span>
    <span class="p">)</span>
    <span class="n">data</span><span class="o">.</span><span class="n">setdefault</span><span class="p">(</span><span class="s2">&quot;weights&quot;</span><span class="p">,</span> <span class="p">[])</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
        <span class="p">[</span><span class="nb">round</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">list</span><span class="p">(</span><span class="n">pipe_poly_ridge</span><span class="o">.</span><span class="n">named_steps</span><span class="p">[</span><span class="s2">&quot;ridge&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">coef_</span><span class="p">[</span><span class="mi">0</span><span class="p">])]</span>
    <span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/Users/kvarada/opt/miniconda3/envs/571/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:147: LinAlgWarning: Ill-conditioned matrix (rcond=3.65967e-18): result may not be accurate.
  return linalg.solve(A, Xy, sym_pos=True,
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>degree</th>
      <th>train_score</th>
      <th>valid_score</th>
      <th>weights</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>0.796</td>
      <td>0.77</td>
      <td>[5.743]</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2</td>
      <td>0.826</td>
      <td>0.731</td>
      <td>[5.743, 0.688]</td>
    </tr>
    <tr>
      <th>2</th>
      <td>4</td>
      <td>0.977</td>
      <td>0.872</td>
      <td>[0.069, 1.806, 0.954, -0.132]</td>
    </tr>
    <tr>
      <th>3</th>
      <td>6</td>
      <td>0.977</td>
      <td>0.871</td>
      <td>[-0.235, 1.737, 1.099, -0.111, -0.013, -0.002]</td>
    </tr>
    <tr>
      <th>4</th>
      <td>15</td>
      <td>0.988</td>
      <td>0.877</td>
      <td>[2.65, -3.544, -23.958, 4.97, 42.755, -1.932, -28.846, 0.378, 9.49, -0.043, -1.621, 0.003, 0.138, -0.0, -0.005]</td>
    </tr>
    <tr>
      <th>5</th>
      <td>20</td>
      <td>1.0</td>
      <td>0.85</td>
      <td>[9.922, -55.976, -114.727, 542.503, 351.067, -1679.367, -478.091, 2333.862, 347.236, -1705.769, -144.523, 708.765, 35.294, -172.262, -4.966, 24.104, 0.371, -1.79, -0.011, 0.054]</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<ul class="simple">
<li><p>When we fit a higher degree polynomial, some weights are large.</p></li>
<li><p>So for higher degree polynomials, the model would be very sensitive to the data.</p></li>
</ul>
</section>
<section id="id3">
<h3>Size of regression weights and overfitting<a class="headerlink" href="#id3" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>The idea of regularization is to “regularize” weights so that they are small and so less sensitive to the data.</p></li>
<li><p>Let’s try adding regularization with a bigger value of <code class="docutils literal notranslate"><span class="pre">alpha</span></code>.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">alpha</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">degrees</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="mi">20</span><span class="p">]</span>
<span class="n">data</span> <span class="o">=</span> <span class="p">{}</span>
<span class="k">for</span> <span class="n">deg</span> <span class="ow">in</span> <span class="n">degrees</span><span class="p">:</span>
    <span class="n">pipe_poly_ridge</span> <span class="o">=</span> <span class="n">make_pipeline</span><span class="p">(</span>
        <span class="n">PolynomialFeatures</span><span class="p">(</span><span class="n">degree</span><span class="o">=</span><span class="n">deg</span><span class="p">,</span> <span class="n">include_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span> <span class="n">Ridge</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">)</span>
    <span class="p">)</span>
    <span class="n">pipe_poly_ridge</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

    <span class="n">data</span><span class="o">.</span><span class="n">setdefault</span><span class="p">(</span><span class="s2">&quot;degree&quot;</span><span class="p">,</span> <span class="p">[])</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">deg</span><span class="p">))</span>
    <span class="n">data</span><span class="o">.</span><span class="n">setdefault</span><span class="p">(</span><span class="s2">&quot;train_score&quot;</span><span class="p">,</span> <span class="p">[])</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
        <span class="nb">str</span><span class="p">(</span><span class="nb">round</span><span class="p">(</span><span class="n">pipe_poly_ridge</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">),</span> <span class="mi">3</span><span class="p">))</span>
    <span class="p">)</span>
    <span class="n">data</span><span class="o">.</span><span class="n">setdefault</span><span class="p">(</span><span class="s2">&quot;valid_score&quot;</span><span class="p">,</span> <span class="p">[])</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
        <span class="nb">str</span><span class="p">(</span><span class="nb">round</span><span class="p">(</span><span class="n">pipe_poly_ridge</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_valid</span><span class="p">,</span> <span class="n">y_valid</span><span class="p">),</span> <span class="mi">3</span><span class="p">))</span>
    <span class="p">)</span>
    <span class="n">data</span><span class="o">.</span><span class="n">setdefault</span><span class="p">(</span><span class="s2">&quot;weights&quot;</span><span class="p">,</span> <span class="p">[])</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
        <span class="p">[</span><span class="nb">round</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">list</span><span class="p">(</span><span class="n">pipe_poly_ridge</span><span class="o">.</span><span class="n">named_steps</span><span class="p">[</span><span class="s2">&quot;ridge&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">coef_</span><span class="p">[</span><span class="mi">0</span><span class="p">])]</span>
    <span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/Users/kvarada/opt/miniconda3/envs/571/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:147: LinAlgWarning: Ill-conditioned matrix (rcond=2.55354e-18): result may not be accurate.
  return linalg.solve(A, Xy, sym_pos=True,
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>degree</th>
      <th>train_score</th>
      <th>valid_score</th>
      <th>weights</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>0.508</td>
      <td>0.467</td>
      <td>[2.29]</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2</td>
      <td>0.534</td>
      <td>0.448</td>
      <td>[2.29, 0.437]</td>
    </tr>
    <tr>
      <th>2</th>
      <td>4</td>
      <td>0.964</td>
      <td>0.879</td>
      <td>[0.124, 0.222, 0.913, 0.04]</td>
    </tr>
    <tr>
      <th>3</th>
      <td>6</td>
      <td>0.966</td>
      <td>0.87</td>
      <td>[0.183, 0.114, 0.534, 0.265, 0.05, -0.025]</td>
    </tr>
    <tr>
      <th>4</th>
      <td>15</td>
      <td>0.977</td>
      <td>0.875</td>
      <td>[0.058, 0.032, 0.097, 0.069, 0.119, 0.106, 0.086, 0.089, -0.016, -0.048, -0.0, 0.007, 0.0, -0.0, -0.0]</td>
    </tr>
    <tr>
      <th>5</th>
      <td>20</td>
      <td>0.979</td>
      <td>0.87</td>
      <td>[0.05, 0.013, 0.087, 0.024, 0.115, 0.031, 0.109, 0.036, 0.04, 0.036, -0.05, 0.02, 0.015, -0.025, -0.002, 0.007, 0.0, -0.001, -0.0, 0.0]</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<ul class="simple">
<li><p>If we “regularize” the weights for a complex model e.g., <span class="math notranslate nohighlight">\(p = 20\)</span> the weights are small</p></li>
<li><p>It means that we are less sensitive to the data.</p></li>
</ul>
</section>
<section id="why-are-small-weights-better">
<h3>Why are small weights better?<a class="headerlink" href="#why-are-small-weights-better" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>Suppose <span class="math notranslate nohighlight">\(x_1\)</span> and <span class="math notranslate nohighlight">\(x_2\)</span> are nearby each other.</p></li>
<li><p>We might expect that they have similar <span class="math notranslate nohighlight">\(\hat{y}\)</span>.</p></li>
<li><p>If we change feature1 value by a small amount <span class="math notranslate nohighlight">\(\epsilon\)</span> in <span class="math notranslate nohighlight">\(x_2\)</span>, leaving everything else the same, we might think that the prediction would be the same.</p></li>
<li><p>But if we have bigger weights small change in <span class="math notranslate nohighlight">\(x_2\)</span> has a large effect on the prediction.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x_1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>
<span class="n">x_2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.8</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>
<span class="n">weights</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">100</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mf">0.22</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;x_1 prediction: &quot;</span><span class="p">,</span> <span class="n">x_1</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">weights</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;x_2 prediction: &quot;</span><span class="p">,</span> <span class="n">x_2</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">weights</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>x_1 prediction:  104.32
x_2 prediction:  84.32
</pre></div>
</div>
</div>
</div>
</section>
<section id="id4">
<h3>Why are small weights better?<a class="headerlink" href="#id4" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>In linear models, the rate of change of the prediction function is proportional to the individual weights.</p></li>
<li><p>So if we want the function to change slowly, we want to ensure that the weights stay small.</p></li>
<li><p>The idea is to avoid putting all our energy into one features, which might give us over-confident predictions and lead to overfitting.</p></li>
</ul>
</section>
<section id="how-alpha-affects-the-weights">
<h3>How <code class="docutils literal notranslate"><span class="pre">alpha</span></code> affects the weights<a class="headerlink" href="#how-alpha-affects-the-weights" title="Permalink to this heading">#</a></h3>
<p>Let’s try different values of alpha and look at the weights.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">deg</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">alphas</span> <span class="o">=</span> <span class="mf">10.0</span> <span class="o">**</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="n">data</span> <span class="o">=</span> <span class="p">{}</span>
<span class="k">for</span> <span class="n">alpha</span> <span class="ow">in</span> <span class="n">alphas</span><span class="p">:</span>
    <span class="n">pipe_poly_ridge</span> <span class="o">=</span> <span class="n">make_pipeline</span><span class="p">(</span>
        <span class="n">PolynomialFeatures</span><span class="p">(</span><span class="n">degree</span><span class="o">=</span><span class="n">deg</span><span class="p">,</span> <span class="n">include_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span> <span class="n">Ridge</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">)</span>
    <span class="p">)</span>
    <span class="n">pipe_poly_ridge</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

    <span class="n">data</span><span class="o">.</span><span class="n">setdefault</span><span class="p">(</span><span class="s2">&quot;alpha&quot;</span><span class="p">,</span> <span class="p">[])</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">alpha</span><span class="p">))</span>
    <span class="n">data</span><span class="o">.</span><span class="n">setdefault</span><span class="p">(</span><span class="s2">&quot;train_score&quot;</span><span class="p">,</span> <span class="p">[])</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
        <span class="nb">str</span><span class="p">(</span><span class="nb">round</span><span class="p">(</span><span class="n">pipe_poly_ridge</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">),</span> <span class="mi">3</span><span class="p">))</span>
    <span class="p">)</span>
    <span class="n">data</span><span class="o">.</span><span class="n">setdefault</span><span class="p">(</span><span class="s2">&quot;valid_score&quot;</span><span class="p">,</span> <span class="p">[])</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
        <span class="nb">str</span><span class="p">(</span><span class="nb">round</span><span class="p">(</span><span class="n">pipe_poly_ridge</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_valid</span><span class="p">,</span> <span class="n">y_valid</span><span class="p">),</span> <span class="mi">3</span><span class="p">))</span>
    <span class="p">)</span>
    <span class="n">data</span><span class="o">.</span><span class="n">setdefault</span><span class="p">(</span><span class="s2">&quot;weights&quot;</span><span class="p">,</span> <span class="p">[])</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
        <span class="p">[</span><span class="nb">round</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">list</span><span class="p">(</span><span class="n">pipe_poly_ridge</span><span class="o">.</span><span class="n">named_steps</span><span class="p">[</span><span class="s2">&quot;ridge&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">coef_</span><span class="p">[</span><span class="mi">0</span><span class="p">])]</span>
    <span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>alpha</th>
      <th>train_score</th>
      <th>valid_score</th>
      <th>weights</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.0001</td>
      <td>0.98</td>
      <td>0.873</td>
      <td>[-0.722, -2.847, 1.487, 3.789, -0.069, -1.175, -0.003, 0.145, 0.001, -0.006]</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.001</td>
      <td>0.98</td>
      <td>0.873</td>
      <td>[-0.719, -2.824, 1.483, 3.77, -0.067, -1.169, -0.003, 0.144, 0.001, -0.006]</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.01</td>
      <td>0.98</td>
      <td>0.873</td>
      <td>[-0.689, -2.608, 1.444, 3.589, -0.053, -1.115, -0.005, 0.137, 0.001, -0.006]</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.1</td>
      <td>0.98</td>
      <td>0.873</td>
      <td>[-0.47, -1.353, 1.161, 2.529, 0.053, -0.792, -0.02, 0.097, 0.001, -0.004]</td>
    </tr>
    <tr>
      <th>4</th>
      <td>1.0</td>
      <td>0.979</td>
      <td>0.874</td>
      <td>[-0.007, 0.205, 0.539, 1.07, 0.288, -0.323, -0.054, 0.036, 0.003, -0.001]</td>
    </tr>
    <tr>
      <th>5</th>
      <td>10.0</td>
      <td>0.977</td>
      <td>0.875</td>
      <td>[0.11, 0.248, 0.285, 0.399, 0.382, -0.011, -0.066, -0.012, 0.004, 0.001]</td>
    </tr>
    <tr>
      <th>6</th>
      <td>100.0</td>
      <td>0.974</td>
      <td>0.872</td>
      <td>[0.076, 0.06, 0.182, 0.122, 0.309, 0.127, -0.039, -0.032, 0.002, 0.002]</td>
    </tr>
    <tr>
      <th>7</th>
      <td>1000.0</td>
      <td>0.957</td>
      <td>0.855</td>
      <td>[0.022, 0.015, 0.053, 0.042, 0.1, 0.073, 0.027, -0.014, -0.003, 0.001]</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<ul class="simple">
<li><p>We also observe the fundamental tradeoff.</p></li>
<li><p>As we increase <code class="docutils literal notranslate"><span class="pre">alpha</span></code> the weights become smaller and smaller.</p></li>
<li><p>We see that the rounded values of some weights are close to zero.</p></li>
</ul>
</section>
<section id="l2-regularization-shrinking-example">
<h3>L2-Regularization “Shrinking” Example<a class="headerlink" href="#l2-regularization-shrinking-example" title="Permalink to this heading">#</a></h3>
<center>
<img src='imgs/l2_shrinking_example.png' width="700" height="700" />
</center>    
<ul class="simple">
<li><p>We get least squares with <span class="math notranslate nohighlight">\(\lambda = 0\)</span>.</p></li>
<li><p>But we can achieve similar training error with smaller <span class="math notranslate nohighlight">\(\lVert w\rVert^2\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(\lVert Xw - y\rVert^2\)</span> increases with <span class="math notranslate nohighlight">\(\lambda\)</span>, and <span class="math notranslate nohighlight">\(\lVert w\rVert^2\)</span> decreases with λ.</p>
<ul>
<li><p>Though individual <span class="math notranslate nohighlight">\(w_j\)</span> can increase or decrease with lambda because we use the L2-norm, the large ones decrease the most.</p></li>
</ul>
</li>
</ul>
</section>
<section id="alpha-values-and-fit-of-the-model">
<h3><code class="docutils literal notranslate"><span class="pre">alpha</span></code> values and fit of the model<a class="headerlink" href="#alpha-values-and-fit-of-the-model" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>Let’s examine the effect of different alpha values on the fit of a polynomial with degree 10.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">plot_alpha_train_score</span><span class="p">():</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
    <span class="n">count</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">alphas</span> <span class="o">=</span> <span class="mf">10.0</span> <span class="o">**</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
    <span class="n">degree</span> <span class="o">=</span> <span class="mi">10</span>
    <span class="k">for</span> <span class="n">alpha</span> <span class="ow">in</span> <span class="n">alphas</span><span class="p">:</span>  <span class="c1"># needs scikit-learn-0.20</span>
        <span class="n">pipe_poly_lr</span> <span class="o">=</span> <span class="n">make_pipeline</span><span class="p">(</span><span class="n">PolynomialFeatures</span><span class="p">(</span><span class="n">degree</span><span class="o">=</span><span class="n">deg</span><span class="p">),</span> <span class="n">Ridge</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">))</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">count</span><span class="p">)</span>
        <span class="n">pipe_poly_lr</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
        <span class="n">preds</span> <span class="o">=</span> <span class="n">pipe_poly_lr</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;blue&quot;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">preds</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;green&quot;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span>
            <span class="s2">&quot;alpha = </span><span class="si">%s</span><span class="s2">, train = </span><span class="si">%0.3f</span><span class="s2">&quot;</span>
            <span class="o">%</span> <span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">alpha</span><span class="p">),</span> <span class="n">pipe_poly_lr</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">))</span>
        <span class="p">)</span>
        <span class="n">count</span> <span class="o">+=</span> <span class="mi">1</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plot_alpha_train_score</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/cf34d3e73d753439727b3995e72e8264a5453a97f68702cdc05348cb9c59e9b5.png" src="../_images/cf34d3e73d753439727b3995e72e8264a5453a97f68702cdc05348cb9c59e9b5.png" />
</div>
</div>
<ul class="simple">
<li><p>Strong regularization means we our complex model becomes smoother.</p></li>
</ul>
</section>
<section id="the-weights-become-smaller-but-never-become-zero">
<h3>The weights become smaller but never become zero<a class="headerlink" href="#the-weights-become-smaller-but-never-become-zero" title="Permalink to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">deg</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">pipe_poly_ridge</span> <span class="o">=</span> <span class="n">make_pipeline</span><span class="p">(</span>
    <span class="n">PolynomialFeatures</span><span class="p">(</span><span class="n">degree</span><span class="o">=</span><span class="n">deg</span><span class="p">,</span> <span class="n">include_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span> <span class="n">Ridge</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
<span class="p">)</span>
<span class="n">pipe_poly_ridge</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">threshold</span> <span class="o">=</span> <span class="mf">1e-6</span>
<span class="n">coeffs</span> <span class="o">=</span> <span class="n">pipe_poly_ridge</span><span class="o">.</span><span class="n">named_steps</span><span class="p">[</span><span class="s2">&quot;ridge&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">coef_</span>
<span class="n">normalized_weights</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">coeffs</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">coeffs</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Features with non-zero values: &quot;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">normalized_weights</span> <span class="o">&gt;</span> <span class="n">threshold</span><span class="p">)[</span><span class="mi">0</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Features with non-zero values:  [0 1 2 3 4 5 6 7 8 9]
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>That said, they won’t have much impact on prediction.</p></li>
<li><p>Strong regularization means the results are not too sensitive to the training data.</p></li>
<li><p>We are keeping the complexity of the model but making it less sensitive to the feature values!</p></li>
</ul>
</section>
<section id="regularization-path">
<h3>Regularization path<a class="headerlink" href="#regularization-path" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>Weights shrink and converge to zero as <span class="math notranslate nohighlight">\(\lambda\)</span> grows but they do not become exactly 0.</p></li>
<li><p>Imagine that <span class="math notranslate nohighlight">\(w_j = 0.000001\)</span>. The penalty for this <span class="math notranslate nohighlight">\(w_j\)</span> is <span class="math notranslate nohighlight">\(0.000000000001\)</span>, i.e., a very tiny number. So there is less incentive to decrease the loss. The incentive (slope) is smaller and smaller as we approach 0 and therefore the coefficients do not become exactly 0.</p></li>
</ul>
<a class="reference internal image-reference" href="../_images/regularization_path.png"><img alt="../_images/regularization_path.png" src="../_images/regularization_path.png" style="width: 700px; height: 700px;" /></a>
</section>
<section id="question-l0-versus-l2-regularization-solution">
<h3>Question: L0 versus L2 regularization (solution)<a class="headerlink" href="#question-l0-versus-l2-regularization-solution" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>Imagine the following two weight vectors which give the same validation errors.
$<span class="math notranslate nohighlight">\(w^1 = \begin{bmatrix}100 \\0\end{bmatrix}  w^2 = \begin{bmatrix}99.98 \\0.03\end{bmatrix}\)</span>$</p></li>
<li><p>Which one would be chosen by L0 regularization?</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(\lVert w^1\rVert_0 = 1\)</span> and <span class="math notranslate nohighlight">\(\lVert w^2\rVert_0 = 2\)</span>. So it will pick <span class="math notranslate nohighlight">\(w^1\)</span></p></li>
</ul>
</li>
<li><p>Which one would be chosen by L2 regularization?</p>
<ul>
<li><p>L2 regularizer focuses on decreasing the largest weight smaller.</p></li>
<li><p><span class="math notranslate nohighlight">\(99.98^2 + 0.03^2 = 9996.0013\)</span> &lt; <span class="math notranslate nohighlight">\(100^2 = 10000\)</span></p></li>
</ul>
</li>
</ul>
</section>
<section id="how-to-pick-lambda">
<h3>How to pick <span class="math notranslate nohighlight">\(\lambda\)</span>?<a class="headerlink" href="#how-to-pick-lambda" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>Theory: as <span class="math notranslate nohighlight">\(n\)</span> grows <span class="math notranslate nohighlight">\(\lambda\)</span> should be in the range <span class="math notranslate nohighlight">\(O(1)\)</span> to <span class="math notranslate nohighlight">\(\sqrt{n}\)</span>.</p></li>
<li><p>Practice: optimize validation set or cross-validation error.</p>
<ul>
<li><p>Almost always decreases the test error.</p></li>
</ul>
</li>
</ul>
</section>
<section id="should-we-regularize-the-y-intercept">
<h3>Should we regularize the y-intercept?<a class="headerlink" href="#should-we-regularize-the-y-intercept" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>No!</p>
<ul>
<li><p>Why encourage it to be closer to zero? (It could be anywhere.)</p></li>
<li><p>You should be allowed to shift function up/down globally.</p></li>
</ul>
</li>
<li><p>Yes!</p>
<ul>
<li><p>Useful for optimization; It makes the solution unique and it easier to compute <span class="math notranslate nohighlight">\(w\)</span></p></li>
</ul>
</li>
<li><p>Compromise: regularize by a smaller amount than other variables.
$<span class="math notranslate nohighlight">\(f(w) = \lVert Xw + w_0 - y\rVert^2 + \frac{\lambda_1}{2}\lVert w\rVert^2 + \frac{\lambda_2}{2}w_0^2\)</span>$</p></li>
</ul>
</section>
<section id="some-properties-of-l2-regularization">
<h3>Some properties of L2 regularization<a class="headerlink" href="#some-properties-of-l2-regularization" title="Permalink to this heading">#</a></h3>
<ol class="arabic simple">
<li><p>Solution <span class="math notranslate nohighlight">\(w\)</span> is unique. (We are not going into mathematical details.)</p></li>
<li><p>Almost always improves the validation error.</p></li>
<li><p>No collinearity issues.</p></li>
<li><p>Less sensitive to changes in <span class="math notranslate nohighlight">\(X\)</span>.</p></li>
<li><p>Gradient descent converges faster (bigger <span class="math notranslate nohighlight">\(\lambda\)</span> means fewer iterations). (You’ll learn about Grafient descent in 572.)</p></li>
<li><p>Worst case: just set <span class="math notranslate nohighlight">\(\lambda\)</span> small and get the same performance</p></li>
</ol>
</section>
<section id="interim-summary-l2-regularization">
<h3>Interim summary: L2 regularization<a class="headerlink" href="#interim-summary-l2-regularization" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>Change the loss function by adding a continuous penalty on the model complexity.</p></li>
<li><p>Best parameter <span class="math notranslate nohighlight">\(\lambda\)</span> almost already leads to improved validation error.</p>
<ul>
<li><p>L2-regularized least squares is also called “ridge regression”.</p></li>
<li><p>Can be solved as a linear system like least squares.</p></li>
</ul>
</li>
<li><p>Some benefits of L2 regularization</p>
<ul>
<li><p>Solution is unique.</p></li>
<li><p>Less sensitive to data.</p></li>
<li><p>Fast.</p></li>
</ul>
</li>
</ul>
</section>
<section id="l1-regularization">
<h3>3. L1 regularization<a class="headerlink" href="#l1-regularization" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>Video 3</p></li>
</ul>
</section>
<section id="id5">
<h3>L1-regularization<a class="headerlink" href="#id5" title="Permalink to this heading">#</a></h3>
<div class="math notranslate nohighlight">
\[f(w) = \frac{1}{2}\sum_i^n(w^TX_i - y_i)^2 + \frac{\lambda}{2}\sum_j^d \lvert w_j \lvert\text{ or }\]</div>
<div class="math notranslate nohighlight">
\[f(w) = \frac{1}{2}\lVert Xw - y\rVert_2^2 + \frac{\lambda}{2} \lVert w\rVert_1\]</div>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\lambda \rightarrow\)</span> regularization strength</p></li>
<li><p><span class="math notranslate nohighlight">\(\lVert w\rVert_1 \rightarrow\)</span>  L1 norm of <span class="math notranslate nohighlight">\(w\)</span></p></li>
<li><p>Objective balances getting low error vs. having small values for <span class="math notranslate nohighlight">\(w_j\)</span></p></li>
</ul>
</section>
<section id="similarities-with-l2-regularization">
<h3>Similarities with L2-regularization<a class="headerlink" href="#similarities-with-l2-regularization" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>L1-regularization
$<span class="math notranslate nohighlight">\(f(w) = \frac{1}{2}\lVert Xw - y\rVert_2^2 + \frac{\lambda}{2} \lVert w\rVert_1\)</span>$</p></li>
<li><p>L2-regularization
$<span class="math notranslate nohighlight">\(f(w) = \frac{1}{2}\lVert Xw - y\rVert_2^2 + \frac{\lambda}{2} \lVert w\rVert_2^2\)</span>$</p></li>
<li><p>Both shrink weights.</p></li>
<li><p>Both result in lower validation error.</p></li>
</ul>
</section>
<section id="terminology-and-notation-ridge-and-lasso">
<h3>Terminology and notation: Ridge and Lasso<a class="headerlink" href="#terminology-and-notation-ridge-and-lasso" title="Permalink to this heading">#</a></h3>
<ul>
<li><p>Linear regression model that uses L2 regularization is called <strong>Ridge</strong> or Tikhonov regularization.</p>
<ul class="simple">
<li><p><a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html?highlight=ridge#sklearn.linear_model.Ridge"><code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code> Ridge</a></p></li>
</ul>
</li>
<li><p>Linear regression model that uses L1 regularization is called <strong>Lasso</strong>.</p>
<ul class="simple">
<li><p><a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html?highlight=lasso"><code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code> Lasso</a></p></li>
</ul>
<blockquote>
<div><p>class sklearn.linear_model.Lasso(alpha=1.0, fit_intercept=True, normalize=False, precompute=False, copy_X=True, max_iter=1000, tol=0.0001, warm_start=False, positive=False, random_state=None, selection=’cyclic’)</p>
</div></blockquote>
</li>
</ul>
<ul class="simple">
<li><p>Let’s use <code class="docutils literal notranslate"><span class="pre">Lasso</span></code> with different values of <code class="docutils literal notranslate"><span class="pre">alpha</span></code>.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">deg</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">alphas</span> <span class="o">=</span> <span class="mf">10.0</span> <span class="o">**</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="n">data</span> <span class="o">=</span> <span class="p">{}</span>
<span class="k">for</span> <span class="n">alpha</span> <span class="ow">in</span> <span class="n">alphas</span><span class="p">:</span>
    <span class="n">pipe_poly_lasso</span> <span class="o">=</span> <span class="n">make_pipeline</span><span class="p">(</span>
        <span class="n">PolynomialFeatures</span><span class="p">(</span><span class="n">degree</span><span class="o">=</span><span class="n">deg</span><span class="p">,</span> <span class="n">include_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
        <span class="n">Lasso</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">1000</span><span class="p">),</span>
    <span class="p">)</span>
    <span class="n">pipe_poly_lasso</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

    <span class="n">data</span><span class="o">.</span><span class="n">setdefault</span><span class="p">(</span><span class="s2">&quot;alpha&quot;</span><span class="p">,</span> <span class="p">[])</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">alpha</span><span class="p">))</span>
    <span class="n">data</span><span class="o">.</span><span class="n">setdefault</span><span class="p">(</span><span class="s2">&quot;train_score&quot;</span><span class="p">,</span> <span class="p">[])</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
        <span class="nb">str</span><span class="p">(</span><span class="nb">round</span><span class="p">(</span><span class="n">pipe_poly_lasso</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">),</span> <span class="mi">3</span><span class="p">))</span>
    <span class="p">)</span>
    <span class="n">data</span><span class="o">.</span><span class="n">setdefault</span><span class="p">(</span><span class="s2">&quot;valid_score&quot;</span><span class="p">,</span> <span class="p">[])</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
        <span class="nb">str</span><span class="p">(</span><span class="nb">round</span><span class="p">(</span><span class="n">pipe_poly_lasso</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_valid</span><span class="p">,</span> <span class="n">y_valid</span><span class="p">),</span> <span class="mi">3</span><span class="p">))</span>
    <span class="p">)</span>
    <span class="n">data</span><span class="o">.</span><span class="n">setdefault</span><span class="p">(</span><span class="s2">&quot;weights&quot;</span><span class="p">,</span> <span class="p">[])</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
        <span class="p">[</span><span class="nb">round</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">list</span><span class="p">(</span><span class="n">pipe_poly_lasso</span><span class="o">.</span><span class="n">named_steps</span><span class="p">[</span><span class="s2">&quot;lasso&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">coef_</span><span class="p">)]</span>
    <span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/Users/kvarada/opt/miniconda3/envs/571/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:529: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 30.391529929329604, tolerance: 0.27492167016359675
  model = cd_fast.enet_coordinate_descent(
/Users/kvarada/opt/miniconda3/envs/571/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:529: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 30.41861102626206, tolerance: 0.27492167016359675
  model = cd_fast.enet_coordinate_descent(
/Users/kvarada/opt/miniconda3/envs/571/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:529: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 30.686729958362754, tolerance: 0.27492167016359675
  model = cd_fast.enet_coordinate_descent(
/Users/kvarada/opt/miniconda3/envs/571/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:529: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 33.61486675943654, tolerance: 0.27492167016359675
  model = cd_fast.enet_coordinate_descent(
/Users/kvarada/opt/miniconda3/envs/571/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:529: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 47.74179289107009, tolerance: 0.27492167016359675
  model = cd_fast.enet_coordinate_descent(
/Users/kvarada/opt/miniconda3/envs/571/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:529: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 24.246535113080938, tolerance: 0.27492167016359675
  model = cd_fast.enet_coordinate_descent(
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>alpha</th>
      <th>train_score</th>
      <th>valid_score</th>
      <th>weights</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.0001</td>
      <td>0.978</td>
      <td>0.873</td>
      <td>[-0.428, 1.632, 1.288, -0.019, -0.045, -0.018, -0.001, 0.0, 0.0, 0.0]</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.001</td>
      <td>0.978</td>
      <td>0.873</td>
      <td>[-0.418, 1.623, 1.282, -0.016, -0.044, -0.018, -0.001, 0.0, 0.0, 0.0]</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.01</td>
      <td>0.978</td>
      <td>0.873</td>
      <td>[-0.313, 1.547, 1.218, 0.013, -0.034, -0.022, -0.001, 0.0, 0.0, 0.0]</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.1</td>
      <td>0.978</td>
      <td>0.874</td>
      <td>[-0.0, 1.058, 1.004, 0.159, 0.001, -0.033, -0.002, -0.0, 0.0, 0.0]</td>
    </tr>
    <tr>
      <th>4</th>
      <td>1.0</td>
      <td>0.972</td>
      <td>0.867</td>
      <td>[0.0, 0.0, 0.434, 0.288, 0.148, -0.006, -0.008, -0.005, -0.0, 0.0]</td>
    </tr>
    <tr>
      <th>5</th>
      <td>10.0</td>
      <td>0.951</td>
      <td>0.853</td>
      <td>[0.0, 0.0, 0.0, 0.0, 0.131, 0.024, 0.019, 0.001, -0.002, -0.0]</td>
    </tr>
    <tr>
      <th>6</th>
      <td>100.0</td>
      <td>0.922</td>
      <td>0.85</td>
      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.041, 0.004, -0.003, -0.0]</td>
    </tr>
    <tr>
      <th>7</th>
      <td>1000.0</td>
      <td>0.764</td>
      <td>0.756</td>
      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.002, 0.0]</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<ul class="simple">
<li><p>Smaller <code class="docutils literal notranslate"><span class="pre">alpha</span></code> values have higher weights</p></li>
<li><p>Larger <code class="docutils literal notranslate"><span class="pre">alpha</span></code> values results in smaller weights.</p></li>
<li><p>Many weights become zero.</p></li>
</ul>
</section>
<section id="the-weights-become-smaller-and-smaller-and-become-zero">
<h3>The weights become smaller and smaller and become zero<a class="headerlink" href="#the-weights-become-smaller-and-smaller-and-become-zero" title="Permalink to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">deg</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">pipe_poly_lasso</span> <span class="o">=</span> <span class="n">make_pipeline</span><span class="p">(</span>
    <span class="n">PolynomialFeatures</span><span class="p">(</span><span class="n">degree</span><span class="o">=</span><span class="n">deg</span><span class="p">,</span> <span class="n">include_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span> <span class="n">Lasso</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="p">)</span>
<span class="n">pipe_poly_lasso</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">threshold</span> <span class="o">=</span> <span class="mf">1e-6</span>
<span class="n">coeffs</span> <span class="o">=</span> <span class="n">pipe_poly_lasso</span><span class="o">.</span><span class="n">named_steps</span><span class="p">[</span><span class="s2">&quot;lasso&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">coef_</span>
<span class="n">normalized_weights</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">coeffs</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">coeffs</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Features with non-zero values: &quot;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">normalized_weights</span> <span class="o">&gt;</span> <span class="n">threshold</span><span class="p">)[</span><span class="mi">0</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Features with non-zero values:  [6 7 8 9]
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>It has assigned zero weight to many features!!</p></li>
<li><p>Sparsity!</p></li>
</ul>
</section>
<section id="terminology-and-notation-sparsity">
<h3>Terminology and notation: Sparsity<a class="headerlink" href="#terminology-and-notation-sparsity" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>We say a linear function is sparse if most of the coefficients are zero.</p></li>
<li><p>Example: Here only 2 out of 8 coefficients are non-zero and so it is a sparse function.
$<span class="math notranslate nohighlight">\(0x_1 + 0.45 x_2 + 0  x_3 + 0x_4 + 1.2x_5 + 0x_6 + 0x_7 + 0x_8\)</span>$</p></li>
</ul>
</section>
<section id="similarities-between-l1-and-l0-regularization">
<h3>Similarities between L1- and L0-regularization<a class="headerlink" href="#similarities-between-l1-and-l0-regularization" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>Like L0-norm, it encourages elements of <span class="math notranslate nohighlight">\(w\)</span> to be exactly zero.</p></li>
<li><p>L1-regularization simultaneously regularizes and selects features.</p></li>
<li><p>Very fast alternative to search and score (L0 penalty).</p>
<ul>
<li><p>With L0 penalty, we had to</p></li>
</ul>
</li>
<li><p>L1-regularization
$<span class="math notranslate nohighlight">\(f(w) = \frac{1}{2}\lVert Xw - y\rVert_2^2 + \frac{\lambda}{2} \lVert w\rVert_1\)</span>$</p></li>
<li><p>L0-regularization
$<span class="math notranslate nohighlight">\(f(w) = \frac{1}{2}\lVert Xw - y\rVert_2^2 + \frac{\lambda}{2} \lVert w\rVert_0\)</span>$</p></li>
</ul>
</section>
<section id="example-l0-vs-l1-vs-l2">
<h3>Example: L0 vs. L1 vs. L2<a class="headerlink" href="#example-l0-vs-l1-vs-l2" title="Permalink to this heading">#</a></h3>
<p>Consider problem where 3 vectors can get minimum training error:</p>
<div class="math notranslate nohighlight">
\[\begin{split}w^1 = \begin{bmatrix}100 \\0.02\end{bmatrix},  w^2 = \begin{bmatrix}100 \\0\end{bmatrix}, w^3 = \begin{bmatrix}99.99 \\0.02\end{bmatrix}\end{split}\]</div>
<ul class="simple">
<li><p>Without regularization, we could choose any of these 3.</p></li>
<li><p>They all have same error, so regularization will “break tie”.</p></li>
<li><p>Which one would you choose with each of L0, L1, L2 regularization?</p></li>
</ul>
</section>
<section id="which-one-would-you-choose-with-l0-regularization">
<h3>Which one would you choose with L0 regularization?<a class="headerlink" href="#which-one-would-you-choose-with-l0-regularization" title="Permalink to this heading">#</a></h3>
<div class="math notranslate nohighlight">
\[\begin{split}w^1 = \begin{bmatrix}100 \\-0.02\end{bmatrix},  w^2 = \begin{bmatrix}100 \\0\end{bmatrix}, w^3 = \begin{bmatrix}99.99 \\0.02\end{bmatrix}\end{split}\]</div>
<ul class="simple">
<li><p>With L0 regularization, you would choose <span class="math notranslate nohighlight">\(w^2\)</span></p>
<ul>
<li><p><span class="math notranslate nohighlight">\(\lVert w^1\rVert_0 = 2\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\lVert w^2\rVert_0 = 1\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\lVert w^3\rVert_0 = 2\)</span></p></li>
</ul>
</li>
</ul>
</section>
<section id="which-one-would-you-choose-with-l1-regularization">
<h3>Which one would you choose with L1 regularization?<a class="headerlink" href="#which-one-would-you-choose-with-l1-regularization" title="Permalink to this heading">#</a></h3>
<div class="math notranslate nohighlight">
\[\begin{split}w^1 = \begin{bmatrix}100 \\0.02\end{bmatrix},  w^2 = \begin{bmatrix}100 \\0\end{bmatrix}, w^3 = \begin{bmatrix}99.99 \\0.02\end{bmatrix}\end{split}\]</div>
<ul class="simple">
<li><p>With L1 regularization, you would choose <span class="math notranslate nohighlight">\(w^2\)</span>.</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(\lVert w^1\rVert_1 = 100.02\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\lVert w^2\rVert_1 = 100\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\lVert w^3\rVert_1 = 100.01\)</span></p></li>
</ul>
</li>
<li><p>L1-regularization focuses on decreasing all <span class="math notranslate nohighlight">\(w_j\)</span> until they are 0.</p></li>
</ul>
</section>
<section id="which-one-would-you-choose-with-l2-regularization">
<h3>Which one would you choose with L2 regularization?<a class="headerlink" href="#which-one-would-you-choose-with-l2-regularization" title="Permalink to this heading">#</a></h3>
<div class="math notranslate nohighlight">
\[\begin{split}w^1 = \begin{bmatrix}100 \\0.02\end{bmatrix},  w^2 = \begin{bmatrix}100 \\0\end{bmatrix}, w^3 = \begin{bmatrix}99.99 \\0.02\end{bmatrix}\end{split}\]</div>
<ul class="simple">
<li><p>With L1 regularization, you would choose <span class="math notranslate nohighlight">\(w^3\)</span>.</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(\lVert w^1\rVert_2^2 = (100)^2 + (0.02)^2 = 10000.0004\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\lVert w^2\rVert_2^2 = (100)^2 = 10000\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\lVert w^3\rVert_2^2 = (99.99)^2 + (0.02)^2 = 9998.0005\)</span></p></li>
</ul>
</li>
<li><p>L2-regularization focuses on decreasing largest <span class="math notranslate nohighlight">\(w_j\)</span> smaller</p></li>
</ul>
</section>
<section id="optional-sparsity-and-regularization">
<h3>(Optional) Sparsity and Regularization<a class="headerlink" href="#optional-sparsity-and-regularization" title="Permalink to this heading">#</a></h3>
<a class="reference internal image-reference" href="../_images/regularization_paths.png"><img alt="../_images/regularization_paths.png" src="../_images/regularization_paths.png" style="width: 1000px; height: 1000px;" /></a>
</section>
<section id="optional-sparsity-and-regularization-with-d-1">
<h3>(optional) Sparsity and Regularization (with d=1)<a class="headerlink" href="#optional-sparsity-and-regularization-with-d-1" title="Permalink to this heading">#</a></h3>
<a class="reference internal image-reference" href="../_images/sparsity_regularization.png"><img alt="../_images/sparsity_regularization.png" src="../_images/sparsity_regularization.png" style="width: 1000px; height: 1000px;" /></a>
</section>
<section id="some-properties-of-l1-regularization">
<h3>Some properties of L1 regularization<a class="headerlink" href="#some-properties-of-l1-regularization" title="Permalink to this heading">#</a></h3>
<ol class="arabic simple">
<li><p>Almost always improves the validation error.</p></li>
<li><p>Can learn with exponential number of irrelevant features.</p></li>
<li><p>Less sensitive to changes in <span class="math notranslate nohighlight">\(X\)</span>.</p></li>
<li><p>The solution is not unique. (If interested in more explanation on this, see slide 43 in <a class="reference external" href="https://www.cs.ubc.ca/~schmidtm/Courses/340-F19/L18.pdf">this slide deck</a>.)</p></li>
</ol>
</section>
<section id="l1-for-feature-selection">
<h3>L1 for feature selection<a class="headerlink" href="#l1-for-feature-selection" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>We used L0 regularization in search and score methods for feature selection.</p></li>
<li><p>Can we use L1 regularization for feature selection because it has sparsity?</p></li>
</ul>
</section>
</section>
<section id="feature-selection-using-l1-regularization">
<h2>4. Feature selection using L1 regularization<a class="headerlink" href="#feature-selection-using-l1-regularization" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p>Video 4</p></li>
</ul>
<section id="feature-selection-methods-so-far">
<h3>Feature selection methods so far<a class="headerlink" href="#feature-selection-methods-so-far" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>Feature selection methods we have seen so far:</p>
<ul>
<li><p>RFE</p></li>
<li><p>Search and score with L0-regularization (e.g., forward search)</p></li>
</ul>
</li>
<li><p>An effective way of feature selection: L1-regularization</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_boston</span><span class="p">,</span> <span class="n">load_breast_cancer</span>

<span class="n">boston_housing</span> <span class="o">=</span> <span class="n">load_boston</span><span class="p">()</span>
<span class="c1">#print(boston_housing.keys())</span>
<span class="c1">#print(boston_housing.DESCR)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">mean_std_cross_val_scores</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns mean and std of cross validation</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">scores</span> <span class="o">=</span> <span class="n">cross_validate</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="n">mean_scores</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">scores</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
    <span class="n">std_scores</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">scores</span><span class="p">)</span><span class="o">.</span><span class="n">std</span><span class="p">()</span>
    <span class="n">out_col</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">mean_scores</span><span class="p">)):</span>
        <span class="n">out_col</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="sa">f</span><span class="s2">&quot;%0.3f (+/- %0.3f)&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">mean_scores</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">std_scores</span><span class="p">[</span><span class="n">i</span><span class="p">])))</span>

    <span class="k">return</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">out_col</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">mean_scores</span><span class="o">.</span><span class="n">index</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">boston_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">boston_housing</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">boston_housing</span><span class="o">.</span><span class="n">feature_names</span><span class="p">)</span>
<span class="n">boston_df</span><span class="p">[</span><span class="s2">&quot;target&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">boston_housing</span><span class="o">.</span><span class="n">target</span>
<span class="n">train_df</span><span class="p">,</span> <span class="n">test_df</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">boston_df</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span> <span class="o">=</span> <span class="n">train_df</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;target&quot;</span><span class="p">]),</span> <span class="n">train_df</span><span class="p">[</span><span class="s2">&quot;target&quot;</span><span class="p">]</span>
<span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">test_df</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;target&quot;</span><span class="p">]),</span> <span class="n">test_df</span><span class="p">[</span><span class="s2">&quot;target&quot;</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">results</span> <span class="o">=</span> <span class="p">{}</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">PolynomialFeatures</span>

<span class="n">pipe_rfe_ridgecv</span> <span class="o">=</span> <span class="n">make_pipeline</span><span class="p">(</span>
    <span class="n">StandardScaler</span><span class="p">(),</span> <span class="n">PolynomialFeatures</span><span class="p">(),</span> <span class="n">RFECV</span><span class="p">(</span><span class="n">Ridge</span><span class="p">(),</span> <span class="n">cv</span><span class="o">=</span><span class="mi">10</span><span class="p">),</span> <span class="n">RidgeCV</span><span class="p">()</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">results</span><span class="p">[</span><span class="s2">&quot;RFE Ridge&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">mean_std_cross_val_scores</span><span class="p">(</span>
    <span class="n">pipe_rfe_ridgecv</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">return_train_score</span><span class="o">=</span><span class="kc">True</span>
<span class="p">)</span>
<span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">results</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>RFE Ridge</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>fit_time</th>
      <td>1.264 (+/- 0.056)</td>
    </tr>
    <tr>
      <th>score_time</th>
      <td>0.002 (+/- 0.000)</td>
    </tr>
    <tr>
      <th>test_score</th>
      <td>0.802 (+/- 0.088)</td>
    </tr>
    <tr>
      <th>train_score</th>
      <td>0.907 (+/- 0.035)</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>Let’s use L1 regularization as feature selection.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.feature_selection</span> <span class="kn">import</span> <span class="n">SelectFromModel</span>

<span class="n">pipe_l1_ridgecv</span> <span class="o">=</span> <span class="n">make_pipeline</span><span class="p">(</span>
    <span class="n">StandardScaler</span><span class="p">(),</span>
    <span class="n">PolynomialFeatures</span><span class="p">(),</span>
    <span class="n">SelectFromModel</span><span class="p">(</span><span class="n">LassoCV</span><span class="p">(</span><span class="n">max_iter</span><span class="o">=</span><span class="mi">2000</span><span class="p">)),</span>
    <span class="n">RidgeCV</span><span class="p">(),</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">results</span><span class="p">[</span><span class="s2">&quot;L1 regularization&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">mean_std_cross_val_scores</span><span class="p">(</span>
    <span class="n">pipe_l1_ridgecv</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">return_train_score</span><span class="o">=</span><span class="kc">True</span>
<span class="p">)</span>
<span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">results</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/Users/kvarada/opt/miniconda3/envs/571/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:525: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.7431972571173446, tolerance: 1.7210772403100776
  model = cd_fast.enet_coordinate_descent_gram(
</pre></div>
</div>
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>RFE Ridge</th>
      <th>L1 regularization</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>fit_time</th>
      <td>1.264 (+/- 0.056)</td>
      <td>0.425 (+/- 0.066)</td>
    </tr>
    <tr>
      <th>score_time</th>
      <td>0.002 (+/- 0.000)</td>
      <td>0.002 (+/- 0.000)</td>
    </tr>
    <tr>
      <th>test_score</th>
      <td>0.802 (+/- 0.088)</td>
      <td>0.815 (+/- 0.078)</td>
    </tr>
    <tr>
      <th>train_score</th>
      <td>0.907 (+/- 0.035)</td>
      <td>0.919 (+/- 0.014)</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>Let’s look at the coefficients.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pipe_l1_ridgecv</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">pipe_l1_ridgecv</span><span class="o">.</span><span class="n">named_steps</span><span class="p">[</span><span class="s2">&quot;selectfrommodel&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">estimator_</span><span class="o">.</span><span class="n">coef_</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([ 0.        , -0.        ,  0.        ,  0.63569027,  0.        ,
       -0.98689837,  3.55026179, -1.31082485, -2.66589432,  0.4751201 ,
       -0.        , -0.59087554,  1.12418015, -3.07261127,  0.10195516,
       -0.        , -0.        ,  5.2006241 , -0.20699814,  0.40540676,
       -0.        , -0.        , -0.47615371,  0.        , -0.        ,
       -0.10305434,  0.433369  ,  0.09021728, -0.31918278, -0.07189749,
       -0.        , -0.0297253 , -0.13775129, -0.2564756 , -0.        ,
        1.41504546, -0.13596046,  0.        , -0.25713116,  0.70304396,
        0.        ,  2.41947452,  0.76452374,  0.18993168,  0.89252337,
       -0.        ,  0.01543774, -0.14731665,  0.        , -1.07087586,
        0.46877825, -1.0369579 , -0.72879662,  0.20370078,  0.        ,
       -0.33684991,  0.        , -0.19279016,  0.0940002 , -0.32156006,
       -0.66742211, -0.17325393, -0.72401858,  1.58968674, -0.80785825,
        0.        , -0.87047052,  0.        ,  0.76079265,  0.08001302,
       -0.89295663,  0.        , -0.6627531 , -1.55126561, -0.74505058,
       -0.28804021, -0.82901689,  0.02147196,  0.        ,  1.57012366,
       -0.27635644, -0.        , -1.66586639, -1.22228607,  1.26817132,
       -1.72117025, -0.79229367,  0.        , -0.20066596,  0.65161591,
       -5.16976194,  4.24458364,  0.        , -0.        , -1.38062283,
       -0.        ,  1.71227319, -0.24527893, -1.19375608,  0.02596607,
       -0.04663238,  0.12756153, -0.33593817, -0.34168376,  0.84770731])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pipe_l1_ridgecv</span><span class="o">.</span><span class="n">named_steps</span><span class="p">[</span><span class="s2">&quot;selectfrommodel&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">threshold_</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>1e-05
</pre></div>
</div>
</div>
</div>
<p>How many features were passed to the final model?</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span>
    <span class="s2">&quot;The number of features after polynomial transformation: &quot;</span><span class="p">,</span>
    <span class="n">pipe_l1_ridgecv</span><span class="o">.</span><span class="n">named_steps</span><span class="p">[</span><span class="s2">&quot;polynomialfeatures&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">n_output_features_</span><span class="p">,</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span>
    <span class="s2">&quot;The number of features passed to the final model: &quot;</span><span class="p">,</span>
    <span class="nb">len</span><span class="p">(</span><span class="n">pipe_l1_ridgecv</span><span class="o">.</span><span class="n">named_steps</span><span class="p">[</span><span class="s2">&quot;ridgecv&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">coef_</span><span class="p">),</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>The number of features after polynomial transformation:  105
The number of features passed to the final model:  77
</pre></div>
</div>
</div>
</div>
</section>
<section id="our-usual-game-of-getting-feature-names-for-the-coefficients">
<h3>Our usual game of getting feature names for the coefficients<a class="headerlink" href="#our-usual-game-of-getting-feature-names-for-the-coefficients" title="Permalink to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="p">(</span>
    <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">pipe_l1_ridgecv</span><span class="o">.</span><span class="n">named_steps</span><span class="p">[</span><span class="s2">&quot;selectfrommodel&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">estimator_</span><span class="o">.</span><span class="n">coef_</span><span class="p">)</span>
    <span class="o">&gt;</span> <span class="n">pipe_l1_ridgecv</span><span class="o">.</span><span class="n">named_steps</span><span class="p">[</span><span class="s2">&quot;selectfrommodel&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">threshold_</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([False, False, False,  True, False,  True,  True,  True,  True,
        True, False,  True,  True,  True,  True, False, False,  True,
        True,  True, False, False,  True, False, False,  True,  True,
        True,  True,  True, False,  True,  True,  True, False,  True,
        True, False,  True,  True, False,  True,  True,  True,  True,
       False,  True,  True, False,  True,  True,  True,  True,  True,
       False,  True, False,  True,  True,  True,  True,  True,  True,
        True,  True, False,  True, False,  True,  True,  True, False,
        True,  True,  True,  True,  True,  True, False,  True,  True,
       False,  True,  True,  True,  True,  True, False,  True,  True,
        True,  True, False, False,  True, False,  True,  True,  True,
        True,  True,  True,  True,  True,  True])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">poly_feature_names</span> <span class="o">=</span> <span class="n">pipe_l1_ridgecv</span><span class="o">.</span><span class="n">named_steps</span><span class="p">[</span>
    <span class="s2">&quot;polynomialfeatures&quot;</span>
<span class="p">]</span><span class="o">.</span><span class="n">get_feature_names</span><span class="p">()</span>  <span class="c1"># transformed list to array</span>
<span class="n">support</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">pipe_l1_ridgecv</span><span class="o">.</span><span class="n">named_steps</span><span class="p">[</span><span class="s2">&quot;selectfrommodel&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">estimator_</span><span class="o">.</span><span class="n">coef_</span><span class="p">)</span>
    <span class="o">&gt;</span> <span class="n">pipe_l1_ridgecv</span><span class="o">.</span><span class="n">named_steps</span><span class="p">[</span><span class="s2">&quot;selectfrommodel&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">threshold_</span>
<span class="p">)</span>
<span class="n">l1_selected_feats</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">poly_feature_names</span><span class="p">)[</span><span class="n">support</span><span class="p">]</span>
<span class="n">l1_selected_feats</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([&#39;x2&#39;, &#39;x4&#39;, &#39;x5&#39;, &#39;x6&#39;, &#39;x7&#39;, &#39;x8&#39;, &#39;x10&#39;, &#39;x11&#39;, &#39;x12&#39;, &#39;x0^2&#39;,
       &#39;x0 x3&#39;, &#39;x0 x4&#39;, &#39;x0 x5&#39;, &#39;x0 x8&#39;, &#39;x0 x11&#39;, &#39;x0 x12&#39;, &#39;x1^2&#39;,
       &#39;x1 x2&#39;, &#39;x1 x3&#39;, &#39;x1 x5&#39;, &#39;x1 x6&#39;, &#39;x1 x7&#39;, &#39;x1 x9&#39;, &#39;x1 x10&#39;,
       &#39;x1 x12&#39;, &#39;x2^2&#39;, &#39;x2 x4&#39;, &#39;x2 x5&#39;, &#39;x2 x6&#39;, &#39;x2 x7&#39;, &#39;x2 x9&#39;,
       &#39;x2 x10&#39;, &#39;x2 x12&#39;, &#39;x3^2&#39;, &#39;x3 x4&#39;, &#39;x3 x5&#39;, &#39;x3 x6&#39;, &#39;x3 x8&#39;,
       &#39;x3 x10&#39;, &#39;x3 x11&#39;, &#39;x3 x12&#39;, &#39;x4^2&#39;, &#39;x4 x5&#39;, &#39;x4 x6&#39;, &#39;x4 x7&#39;,
       &#39;x4 x8&#39;, &#39;x4 x10&#39;, &#39;x4 x12&#39;, &#39;x5^2&#39;, &#39;x5 x6&#39;, &#39;x5 x8&#39;, &#39;x5 x9&#39;,
       &#39;x5 x10&#39;, &#39;x5 x11&#39;, &#39;x5 x12&#39;, &#39;x6^2&#39;, &#39;x6 x8&#39;, &#39;x6 x9&#39;, &#39;x6 x11&#39;,
       &#39;x6 x12&#39;, &#39;x7^2&#39;, &#39;x7 x8&#39;, &#39;x7 x9&#39;, &#39;x7 x11&#39;, &#39;x7 x12&#39;, &#39;x8^2&#39;,
       &#39;x8 x9&#39;, &#39;x8 x12&#39;, &#39;x9 x10&#39;, &#39;x9 x11&#39;, &#39;x9 x12&#39;, &#39;x10^2&#39;,
       &#39;x10 x11&#39;, &#39;x10 x12&#39;, &#39;x11^2&#39;, &#39;x11 x12&#39;, &#39;x12^2&#39;], dtype=&#39;&lt;U7&#39;)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">coef_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
    <span class="n">data</span><span class="o">=</span><span class="n">pipe_l1_ridgecv</span><span class="o">.</span><span class="n">named_steps</span><span class="p">[</span><span class="s2">&quot;ridgecv&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">coef_</span><span class="p">,</span>
    <span class="n">index</span><span class="o">=</span><span class="n">l1_selected_feats</span><span class="p">,</span>
    <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;Coefficient&quot;</span><span class="p">],</span>
<span class="p">)</span>
<span class="n">coef_df</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Coefficient</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>x2</th>
      <td>1.464352</td>
    </tr>
    <tr>
      <th>x4</th>
      <td>-1.013904</td>
    </tr>
    <tr>
      <th>x5</th>
      <td>3.604779</td>
    </tr>
    <tr>
      <th>x6</th>
      <td>-1.601881</td>
    </tr>
    <tr>
      <th>x7</th>
      <td>-2.430137</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
    </tr>
    <tr>
      <th>x10 x11</th>
      <td>0.065983</td>
    </tr>
    <tr>
      <th>x10 x12</th>
      <td>0.269627</td>
    </tr>
    <tr>
      <th>x11^2</th>
      <td>-0.305546</td>
    </tr>
    <tr>
      <th>x11 x12</th>
      <td>-0.287693</td>
    </tr>
    <tr>
      <th>x12^2</th>
      <td>0.737603</td>
    </tr>
  </tbody>
</table>
<p>77 rows × 1 columns</p>
</div></div></div>
</div>
<p>Note: Although L2 regularization doesn’t make coefficients zero, you can still use it for feature selection using <code class="docutils literal notranslate"><span class="pre">SelectFromModel</span></code> with a threshold. See documentation <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectFromModel.html">here</a>.</p>
</section>
<section id="regularized-logistic-regression">
<h3>Regularized logistic regression<a class="headerlink" href="#regularized-logistic-regression" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>Regularization is not limited to least squares.</p></li>
<li><p>We can add L1 and L2 penalty terms in other loss functions as well.</p></li>
<li><p>Let’s look at logistic regression with L1 and L2 regularization.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_breast_cancer</span>

<span class="n">breast_cancer</span> <span class="o">=</span> <span class="n">load_breast_cancer</span><span class="p">()</span>
<span class="c1">#print(breast_cancer.keys())</span>
<span class="c1">#print(breast_cancer.DESCR)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">breast_cancer_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">breast_cancer</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">breast_cancer</span><span class="o">.</span><span class="n">feature_names</span><span class="p">)</span>
<span class="n">breast_cancer_df</span><span class="p">[</span><span class="s2">&quot;target&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">breast_cancer</span><span class="o">.</span><span class="n">target</span>
<span class="n">train_df</span><span class="p">,</span> <span class="n">test_df</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">breast_cancer_df</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span> <span class="o">=</span> <span class="n">train_df</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;target&quot;</span><span class="p">]),</span> <span class="n">train_df</span><span class="p">[</span><span class="s2">&quot;target&quot;</span><span class="p">]</span>
<span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">test_df</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;target&quot;</span><span class="p">]),</span> <span class="n">test_df</span><span class="p">[</span><span class="s2">&quot;target&quot;</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_train</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>mean radius</th>
      <th>mean texture</th>
      <th>mean perimeter</th>
      <th>mean area</th>
      <th>mean smoothness</th>
      <th>mean compactness</th>
      <th>mean concavity</th>
      <th>mean concave points</th>
      <th>mean symmetry</th>
      <th>mean fractal dimension</th>
      <th>...</th>
      <th>worst radius</th>
      <th>worst texture</th>
      <th>worst perimeter</th>
      <th>worst area</th>
      <th>worst smoothness</th>
      <th>worst compactness</th>
      <th>worst concavity</th>
      <th>worst concave points</th>
      <th>worst symmetry</th>
      <th>worst fractal dimension</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>560</th>
      <td>14.05</td>
      <td>27.15</td>
      <td>91.38</td>
      <td>600.4</td>
      <td>0.09929</td>
      <td>0.11260</td>
      <td>0.04462</td>
      <td>0.04304</td>
      <td>0.1537</td>
      <td>0.06171</td>
      <td>...</td>
      <td>15.30</td>
      <td>33.17</td>
      <td>100.20</td>
      <td>706.7</td>
      <td>0.1241</td>
      <td>0.22640</td>
      <td>0.1326</td>
      <td>0.10480</td>
      <td>0.2250</td>
      <td>0.08321</td>
    </tr>
    <tr>
      <th>428</th>
      <td>11.13</td>
      <td>16.62</td>
      <td>70.47</td>
      <td>381.1</td>
      <td>0.08151</td>
      <td>0.03834</td>
      <td>0.01369</td>
      <td>0.01370</td>
      <td>0.1511</td>
      <td>0.06148</td>
      <td>...</td>
      <td>11.68</td>
      <td>20.29</td>
      <td>74.35</td>
      <td>421.1</td>
      <td>0.1030</td>
      <td>0.06219</td>
      <td>0.0458</td>
      <td>0.04044</td>
      <td>0.2383</td>
      <td>0.07083</td>
    </tr>
    <tr>
      <th>198</th>
      <td>19.18</td>
      <td>22.49</td>
      <td>127.50</td>
      <td>1148.0</td>
      <td>0.08523</td>
      <td>0.14280</td>
      <td>0.11140</td>
      <td>0.06772</td>
      <td>0.1767</td>
      <td>0.05529</td>
      <td>...</td>
      <td>23.36</td>
      <td>32.06</td>
      <td>166.40</td>
      <td>1688.0</td>
      <td>0.1322</td>
      <td>0.56010</td>
      <td>0.3865</td>
      <td>0.17080</td>
      <td>0.3193</td>
      <td>0.09221</td>
    </tr>
    <tr>
      <th>203</th>
      <td>13.81</td>
      <td>23.75</td>
      <td>91.56</td>
      <td>597.8</td>
      <td>0.13230</td>
      <td>0.17680</td>
      <td>0.15580</td>
      <td>0.09176</td>
      <td>0.2251</td>
      <td>0.07421</td>
      <td>...</td>
      <td>19.20</td>
      <td>41.85</td>
      <td>128.50</td>
      <td>1153.0</td>
      <td>0.2226</td>
      <td>0.52090</td>
      <td>0.4646</td>
      <td>0.20130</td>
      <td>0.4432</td>
      <td>0.10860</td>
    </tr>
    <tr>
      <th>41</th>
      <td>10.95</td>
      <td>21.35</td>
      <td>71.90</td>
      <td>371.1</td>
      <td>0.12270</td>
      <td>0.12180</td>
      <td>0.10440</td>
      <td>0.05669</td>
      <td>0.1895</td>
      <td>0.06870</td>
      <td>...</td>
      <td>12.84</td>
      <td>35.34</td>
      <td>87.22</td>
      <td>514.0</td>
      <td>0.1909</td>
      <td>0.26980</td>
      <td>0.4023</td>
      <td>0.14240</td>
      <td>0.2964</td>
      <td>0.09606</td>
    </tr>
  </tbody>
</table>
<p>5 rows × 30 columns</p>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">y_train</span><span class="o">.</span><span class="n">value_counts</span><span class="p">(</span><span class="n">normalize</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>1    0.632967
0    0.367033
Name: target, dtype: float64
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">f1_score</span><span class="p">,</span> <span class="n">recall_score</span><span class="p">,</span> <span class="n">make_scorer</span>

<span class="n">custom_scorer</span> <span class="o">=</span> <span class="n">make_scorer</span><span class="p">(</span><span class="n">f1_score</span><span class="p">,</span> <span class="n">pos_label</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">scoring_metric</span> <span class="o">=</span> <span class="n">custom_scorer</span>

<span class="n">results_classification</span> <span class="o">=</span> <span class="p">{}</span>
</pre></div>
</div>
</div>
</div>
<p>Let’s try <code class="docutils literal notranslate"><span class="pre">DummyClassifier</span></code></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">dummy</span> <span class="o">=</span> <span class="n">DummyClassifier</span><span class="p">()</span>
<span class="n">results_classification</span><span class="p">[</span><span class="s2">&quot;dummy&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">mean_std_cross_val_scores</span><span class="p">(</span>
    <span class="n">dummy</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">return_train_score</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="n">scoring_metric</span><span class="p">,</span> 
<span class="p">)</span>
<span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">results_classification</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/Users/kvarada/opt/miniconda3/envs/571/lib/python3.8/site-packages/sklearn/dummy.py:131: FutureWarning: The default value of strategy will change from stratified to prior in 0.24.
  warnings.warn(&quot;The default value of strategy will change from &quot;
</pre></div>
</div>
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>dummy</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>fit_time</th>
      <td>0.001 (+/- 0.001)</td>
    </tr>
    <tr>
      <th>score_time</th>
      <td>0.001 (+/- 0.001)</td>
    </tr>
    <tr>
      <th>test_score</th>
      <td>0.324 (+/- 0.102)</td>
    </tr>
    <tr>
      <th>train_score</th>
      <td>0.356 (+/- 0.018)</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
</section>
<section id="logistic-regression-with-regularization">
<h3>Logistic regression with regularization<a class="headerlink" href="#logistic-regression-with-regularization" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>In <code class="docutils literal notranslate"><span class="pre">sklearn</span></code>, by default logistic regression uses L2 regularization.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pipe_lgr_l2</span> <span class="o">=</span> <span class="n">make_pipeline</span><span class="p">(</span><span class="n">StandardScaler</span><span class="p">(),</span> <span class="n">LogisticRegression</span><span class="p">())</span>
<span class="n">results_classification</span><span class="p">[</span><span class="s2">&quot;Logistic Regression L2&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">mean_std_cross_val_scores</span><span class="p">(</span>
    <span class="n">pipe_lgr_l2</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">return_train_score</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="n">scoring_metric</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">results_classification</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>dummy</th>
      <th>Logistic Regression L2</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>fit_time</th>
      <td>0.001 (+/- 0.001)</td>
      <td>0.012 (+/- 0.003)</td>
    </tr>
    <tr>
      <th>score_time</th>
      <td>0.001 (+/- 0.001)</td>
      <td>0.002 (+/- 0.000)</td>
    </tr>
    <tr>
      <th>test_score</th>
      <td>0.324 (+/- 0.102)</td>
      <td>0.970 (+/- 0.011)</td>
    </tr>
    <tr>
      <th>train_score</th>
      <td>0.356 (+/- 0.018)</td>
      <td>0.985 (+/- 0.005)</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>All coefficients are non-zero.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pipe_lgr_l2</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">pipe_lgr_l2</span><span class="o">.</span><span class="n">named_steps</span><span class="p">[</span><span class="s2">&quot;logisticregression&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">coef_</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([[-0.53985391, -0.25214061, -0.49456043, -0.61045146, -0.17406619,
         0.57798312, -0.65877728, -0.98024393,  0.11997381,  0.40647904,
        -1.24795634, -0.10325774, -0.73120148, -0.93158422, -0.23099107,
         0.61431692, -0.03231823, -0.20637638,  0.27288293,  0.73956073,
        -1.04015025, -1.09620691, -0.88548864, -1.02686306, -0.92729009,
        -0.07540269, -0.76656759, -0.73296575, -0.66992505, -0.52093938]])
</pre></div>
</div>
</div>
</div>
<p>Let’s try logistic regression with L1 regularization</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pipe_lgr_l1</span> <span class="o">=</span> <span class="n">make_pipeline</span><span class="p">(</span>
    <span class="n">StandardScaler</span><span class="p">(),</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">solver</span><span class="o">=</span><span class="s2">&quot;liblinear&quot;</span><span class="p">,</span> <span class="n">penalty</span><span class="o">=</span><span class="s2">&quot;l1&quot;</span><span class="p">)</span>
<span class="p">)</span>
<span class="n">results_classification</span><span class="p">[</span><span class="s2">&quot;Logistic Regression L1&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">mean_std_cross_val_scores</span><span class="p">(</span>
    <span class="n">pipe_lgr_l1</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">return_train_score</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="n">scoring_metric</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">results_classification</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>dummy</th>
      <th>Logistic Regression L2</th>
      <th>Logistic Regression L1</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>fit_time</th>
      <td>0.001 (+/- 0.001)</td>
      <td>0.012 (+/- 0.003)</td>
      <td>0.005 (+/- 0.000)</td>
    </tr>
    <tr>
      <th>score_time</th>
      <td>0.001 (+/- 0.001)</td>
      <td>0.002 (+/- 0.000)</td>
      <td>0.002 (+/- 0.000)</td>
    </tr>
    <tr>
      <th>test_score</th>
      <td>0.324 (+/- 0.102)</td>
      <td>0.970 (+/- 0.011)</td>
      <td>0.967 (+/- 0.007)</td>
    </tr>
    <tr>
      <th>train_score</th>
      <td>0.356 (+/- 0.018)</td>
      <td>0.985 (+/- 0.005)</td>
      <td>0.984 (+/- 0.006)</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<ul class="simple">
<li><p>The scores are more or less the same.</p></li>
<li><p>But L1 regularization is carrying out feature selection.</p></li>
<li><p>Many coefficients are 0.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pipe_lgr_l1</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">pipe_lgr_l1</span><span class="o">.</span><span class="n">named_steps</span><span class="p">[</span><span class="s2">&quot;logisticregression&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">coef_</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([[ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
         0.        ,  0.        , -1.34879417,  0.        ,  0.26748365,
        -2.02931667,  0.        ,  0.        ,  0.        , -0.12686138,
         0.55076679,  0.        ,  0.        ,  0.        ,  0.21862455,
        -1.65562392, -1.37553893, -0.29633874, -3.44417943, -0.98542359,
         0.        , -1.03509447, -0.63533194, -0.29819291,  0.        ]])
</pre></div>
</div>
</div>
</div>
<p>We can also carry out feature selection using L1 regularization and pass selected features to another model.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">lightgbm.sklearn</span> <span class="kn">import</span> <span class="n">LGBMClassifier</span>
<span class="n">pipe_lgr_lgbm</span> <span class="o">=</span> <span class="n">make_pipeline</span><span class="p">(</span>
    <span class="n">StandardScaler</span><span class="p">(),</span>
    <span class="n">SelectFromModel</span><span class="p">(</span><span class="n">LogisticRegression</span><span class="p">(</span><span class="n">solver</span><span class="o">=</span><span class="s2">&quot;liblinear&quot;</span><span class="p">,</span> <span class="n">penalty</span><span class="o">=</span><span class="s2">&quot;l1&quot;</span><span class="p">)),</span>
    <span class="n">LGBMClassifier</span><span class="p">(),</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">results_classification</span><span class="p">[</span><span class="s2">&quot;L1 + LGBM&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">mean_std_cross_val_scores</span><span class="p">(</span>
    <span class="n">pipe_lgr_lgbm</span><span class="p">,</span>
    <span class="n">X_train</span><span class="p">,</span>
    <span class="n">y_train</span><span class="p">,</span>
    <span class="n">return_train_score</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">scoring</span><span class="o">=</span><span class="n">scoring_metric</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">results_classification</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>dummy</th>
      <th>Logistic Regression L2</th>
      <th>Logistic Regression L1</th>
      <th>L1 + LGBM</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>fit_time</th>
      <td>0.001 (+/- 0.001)</td>
      <td>0.012 (+/- 0.003)</td>
      <td>0.005 (+/- 0.000)</td>
      <td>0.039 (+/- 0.009)</td>
    </tr>
    <tr>
      <th>score_time</th>
      <td>0.001 (+/- 0.001)</td>
      <td>0.002 (+/- 0.000)</td>
      <td>0.002 (+/- 0.000)</td>
      <td>0.003 (+/- 0.000)</td>
    </tr>
    <tr>
      <th>test_score</th>
      <td>0.324 (+/- 0.102)</td>
      <td>0.970 (+/- 0.011)</td>
      <td>0.967 (+/- 0.007)</td>
      <td>0.961 (+/- 0.037)</td>
    </tr>
    <tr>
      <th>train_score</th>
      <td>0.356 (+/- 0.018)</td>
      <td>0.985 (+/- 0.005)</td>
      <td>0.984 (+/- 0.006)</td>
      <td>1.000 (+/- 0.000)</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<ul class="simple">
<li><p>The score went down in this case. But this might help in some other cases.</p></li>
<li><p>The resulting model is using L1 selected features only.</p></li>
</ul>
</section>
</section>
<section id="regularization-scaling-and-colinearity">
<h2>5. Regularization: scaling and colinearity<a class="headerlink" href="#regularization-scaling-and-colinearity" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p>Video 5</p></li>
</ul>
<section id="regularization-and-scaling">
<h3>Regularization and scaling<a class="headerlink" href="#regularization-and-scaling" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>It doesn’t matter for decision trees or naive Bayes.</p>
<ul>
<li><p>They only look at one feature at a time.</p></li>
</ul>
</li>
<li><p>It doesn’t matter for least squares:</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(w_j*(100 mL)\)</span> gives the same model as <span class="math notranslate nohighlight">\(w_j*(0.1 L)\)</span> with a different <span class="math notranslate nohighlight">\(w_j\)</span></p></li>
</ul>
</li>
<li><p>It matters for <span class="math notranslate nohighlight">\(k\)</span>-nearest neighbours:</p>
<ul>
<li><p>Distance will be affected more by large features than small features.</p></li>
</ul>
</li>
<li><p><strong>It matters for regularized least squares</strong>:</p>
<ul>
<li><p>Penalizing <span class="math notranslate nohighlight">\(w_j^2\)</span> means different things if features <span class="math notranslate nohighlight">\(j\)</span> are on different scales</p></li>
<li><p>Penalizing <span class="math notranslate nohighlight">\(w_j\)</span> means different things if features <span class="math notranslate nohighlight">\(j\)</span> are on different scales</p></li>
</ul>
</li>
</ul>
</section>
</section>
<section id="scaling-features">
<h2>Scaling features<a class="headerlink" href="#scaling-features" title="Permalink to this heading">#</a></h2>
<p>We’ve seen cases where scaling the features is important to get good performance. We can test this out:</p>
<section id="normalizing-linearregression">
<h3>Normalizing <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html">LinearRegression</a><a class="headerlink" href="#normalizing-linearregression" title="Permalink to this heading">#</a></h3>
<blockquote>
<div><p>normalize: bool, optional, default False
This parameter is ignored when fit_intercept is set to False. If True, the regressors X will be normalized before regression by subtracting the mean and dividing by the l2-norm. If you wish to standardize, please use sklearn.preprocessing.StandardScaler before calling fit on an estimator with normalize=False.</p>
</div></blockquote>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_regression</span>
<span class="n">n_samples</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">n_features</span> <span class="o">=</span> <span class="mi">10</span>

<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_regression</span><span class="p">(</span>
    <span class="n">n_samples</span><span class="o">=</span><span class="n">n_samples</span><span class="p">,</span>
    <span class="n">n_features</span><span class="o">=</span><span class="n">n_features</span><span class="p">,</span>
    <span class="n">n_informative</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
    <span class="n">noise</span><span class="o">=</span><span class="mf">0.50</span><span class="p">,</span>
    <span class="n">random_state</span><span class="o">=</span><span class="mi">111</span><span class="p">,</span>
<span class="p">)</span>
<span class="c1">### make the first feature on a huge scale</span>
<span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="mi">1000</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span>
    <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.20</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">111</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Let’s check whether the linear regression model is different when we normalize vs. when we do not normalize</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">lr</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">(</span><span class="n">normalize</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">lr</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="n">lr_scale</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">(</span><span class="n">normalize</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">lr_scale</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="n">np</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">lr</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">),</span> <span class="n">lr_scale</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>True
</pre></div>
</div>
</div>
</div>
<p>Let’s check whether the Ridge model is different when we normalize vs. when we do not normalize</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">ridge</span> <span class="o">=</span> <span class="n">Ridge</span><span class="p">(</span><span class="n">normalize</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">ridge</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="n">ridge_scale</span> <span class="o">=</span> <span class="n">Ridge</span><span class="p">(</span><span class="n">normalize</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">ridge_scale</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="n">np</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">ridge</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">),</span> <span class="n">ridge_scale</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>False
</pre></div>
</div>
</div>
</div>
<p>Let’s check whether the Lasso model is different when we normalize vs. when we do not normalize</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">lasso</span> <span class="o">=</span> <span class="n">Lasso</span><span class="p">(</span><span class="n">normalize</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">lasso</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="n">lasso_scale</span> <span class="o">=</span> <span class="n">Lasso</span><span class="p">(</span><span class="n">normalize</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">lasso_scale</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="n">np</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">lasso</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">),</span> <span class="n">lasso_scale</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>False
</pre></div>
</div>
</div>
</div>
</section>
<section id="scaling-summary">
<h3>Scaling: summary<a class="headerlink" href="#scaling-summary" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>Un-regularized linear regression: not affected by scaling</p></li>
<li><p>L1 or L2-regularized linear regression: both affected by scaling (and it’s usually a good idea)</p></li>
</ul>
</section>
<section id="collinearity-and-regularization">
<h3>Collinearity and regularization<a class="headerlink" href="#collinearity-and-regularization" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>If you have colinear features, the weights would go crazy with regular linear regression.</p></li>
<li><p>With L2 regularization: The weight will be equally distributed among all collinear features because the solution is unique.</p>
<ul>
<li><p>Example: suppose we have three identical features with a total weight of 1</p></li>
<li><p>The weight will be distributed as 1/3, 1/3, 1/3 among the features.</p></li>
</ul>
</li>
<li><p>With L1 regularization: The weight will not be equally distributed; the solution is not unique.</p>
<ul>
<li><p>Example: suppose we have three identical features with a total weight of 1</p></li>
<li><p>The weight could be distributed in many different ways</p></li>
<li><p>For example, 1/2, 1/4, 1/4 or 1.0, 0, 0 or 1/2, 1/2, 0 and so on …</p></li>
</ul>
</li>
</ul>
</section>
<section id="elastic-nets">
<h3>Elastic nets<a class="headerlink" href="#elastic-nets" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>Combine good properties from both worlds</p></li>
</ul>
<div class="math notranslate nohighlight">
\[f(w) = \frac{1}{2}\lVert Xw - y\rVert_2^2 + \lambda(\alpha\lVert w\rVert_1 + (1-\alpha)\lVert w\rVert_2^2 )  \]</div>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\lambda\)</span> control the strength of regularization</p></li>
<li><p><span class="math notranslate nohighlight">\(\alpha\)</span> controls the amount of sparsity and smoothness</p></li>
<li><p>L1 promotes sparsity and the L2 promotes smoothness.</p></li>
<li><p>The functional is strictly convex: the solution is unique.</p></li>
<li><p>No collinearity problem</p>
<ul>
<li><p>A whole group of correlated variables is selected rather than just one variable in the group.</p></li>
</ul>
</li>
</ul>
<p>You can use elastic nets using sklearn’s <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html">ElasticNet</a>.</p>
</section>
<section id="how-to-use-regularization-with-scikit-learn-some-examples">
<h3>How to use regularization with <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code>: some examples<a class="headerlink" href="#how-to-use-regularization-with-scikit-learn-some-examples" title="Permalink to this heading">#</a></h3>
<ul>
<li><p>Regression</p>
<ul class="simple">
<li><p>Least squares with L2-regularization: <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html">Ridge</a></p></li>
<li><p>Least squares with L1-regularization: <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html">Lasso</a></p></li>
<li><p>Least squares with L1- and L2-regularization: <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html">ElasticNet</a></p></li>
<li><p><a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html?highlight=svr#sklearn.svm.SVR">SVR</a> (<span class="math notranslate nohighlight">\(\epsilon\)</span>-insensitive loss function)</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">epsilon</span> <span class="pre">=</span> <span class="pre">0</span></code> gives us <code class="docutils literal notranslate"><span class="pre">KernelRidge</span></code> model (least squares with RBF)</p></li>
</ul>
</li>
</ul>
</li>
<li><p>Classification</p>
<ul class="simple">
<li><p><a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html">SVC</a> (supports L2-regularization)</p></li>
<li><p><a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression">LogisticRegression</a> (support L1 and L2 with different solvers)</p></li>
</ul>
<blockquote>
<div><p>penalty{‘l1’, ‘l2’, ‘elasticnet’, ‘none’}, default=’l2’ Used to specify the norm used in the penalization. The ‘newton-cg’, ‘sag’ and ‘lbfgs’ solvers support only l2 penalties. ‘elasticnet’ is only supported by the ‘saga’ solver. If ‘none’ (not supported by the liblinear solver), no regularization is applied.</p>
</div></blockquote>
</li>
</ul>
<section id="summary-l-regularization">
<h4>Summary: L* regularization<a class="headerlink" href="#summary-l-regularization" title="Permalink to this heading">#</a></h4>
<ul class="simple">
<li><p>L0-regularization (AIC, BIC, Mallow’s Cp, Adjusted R2, ANOVA):</p>
<ul>
<li><p>Adds penalty on the number of non-zeros to select features.</p></li>
</ul>
</li>
</ul>
<div class="math notranslate nohighlight">
\[f(w) = \frac{1}{2}\lVert Xw - y\rVert_2^2 + \lambda\lVert w\rVert_0\]</div>
<ul class="simple">
<li><p>L2-regularization (ridge regression):</p>
<ul>
<li><p>Adding penalty on the L2-norm of <span class="math notranslate nohighlight">\(w\)</span> to decrease overfitting:
$<span class="math notranslate nohighlight">\(f(w) = \frac{1}{2}\lVert Xw - y\rVert_2^2 + \frac{\lambda}{2} \lVert w\rVert_2^2\)</span>$</p></li>
</ul>
</li>
<li><p>L1-regularization (lasso regression):</p>
<ul>
<li><p>Adding penalty on the L1-norm decreases overfitting and selects features:
$<span class="math notranslate nohighlight">\(f(w) = \frac{1}{2}\lVert Xw - y\rVert_2^2 + \frac{\lambda}{2} \lVert w\rVert_1\)</span>$</p></li>
</ul>
</li>
</ul>
</section>
<section id="id6">
<h4>Summary: L* regularization<a class="headerlink" href="#id6" title="Permalink to this heading">#</a></h4>
<br>
<a class="reference internal image-reference" href="../_images/regularization_summary.png"><img alt="../_images/regularization_summary.png" src="../_images/regularization_summary.png" style="width: 1000px; height: 1000px;" /></a>
<ul class="simple">
<li><p>“Elastic net” (L1- and L2-regularization) is sparse, fast, and unique.</p></li>
</ul>
</section>
</section>
<section id="true-false-questions-l2-regularization">
<h3>True/False questions: L2-regularization<a class="headerlink" href="#true-false-questions-l2-regularization" title="Permalink to this heading">#</a></h3>
<ol class="arabic simple">
<li><p>Introducing L2 regularization to the model means making it less sensitive to changes in <span class="math notranslate nohighlight">\(X\)</span>. (True)</p></li>
<li><p>Introducing L2 regularization to the model can results in worse performance on the training set. (True)</p></li>
<li><p>Imagine that you fit linear regression twice with different values of <span class="math notranslate nohighlight">\(\lambda\)</span>, <span class="math notranslate nohighlight">\(\lambda = 0\)</span> and <span class="math notranslate nohighlight">\(\lambda=10\)</span>. You are given the weights learned from two different models below. Without knowing which weights came from which model, you can guess that <span class="math notranslate nohighlight">\(w^1\)</span> probably corresponds to <span class="math notranslate nohighlight">\(\lambda = 10\)</span> and <span class="math notranslate nohighlight">\(w^2\)</span> probably corresponds to <span class="math notranslate nohighlight">\(\lambda = 0\)</span>. (False. Using higher <span class="math notranslate nohighlight">\(\lambda\)</span> is likely to result in lower weights.)
$<span class="math notranslate nohighlight">\(w^1 = \begin{bmatrix} 32.43\\23.14\\16.4\end{bmatrix} \text{ and }w^2 = \begin{bmatrix} 2.03\\1.2\\0.4\end{bmatrix}\)</span>$</p></li>
<li><p>L2 regularization shrinks the weights but all <span class="math notranslate nohighlight">\(w_j\)</span>s tend to be non-zero. (True)</p></li>
<li><p>In L2 regularization, as <span class="math notranslate nohighlight">\(\lambda\)</span> increases, <span class="math notranslate nohighlight">\(\lVert Xw –y\rVert_2^2\)</span> decreases and <span class="math notranslate nohighlight">\(\lVert w\rVert_2^2\)</span> increases. (False)</p></li>
</ol>
<div class="math notranslate nohighlight">
\[f(w) = \frac{1}{2}\lVert Xw - y\rVert_2^2 + \frac{\lambda}{2} \lVert w\rVert_2^2\]</div>
<p><br><br><br><br><br><br><br><br><br></p>
</section>
<section id="true-false-questions-l1-regularization">
<h3>True/False questions: L1-regularization<a class="headerlink" href="#true-false-questions-l1-regularization" title="Permalink to this heading">#</a></h3>
<ol class="arabic simple">
<li><p>Both L0- and L1-regularization give sparsity but L2-regularization doesn’t. (True)</p></li>
<li><p>In L1-regularization, larger <span class="math notranslate nohighlight">\(\lambda\)</span> selects few features and smaller <span class="math notranslate nohighlight">\(\lambda\)</span> allows many features. (True)
$<span class="math notranslate nohighlight">\(f(w) = \frac{1}{2}\lVert Xw - y\rVert_2^2 + \frac{\lambda}{2} \lVert w\rVert_1\)</span>$</p></li>
<li><p>L1-regularization tends to be a better choice than L2 regularization in most cases. (False. It’s context dependent.)</p></li>
<li><p>Both L1- and L2-regularization shrink weights. (True)</p></li>
</ol>
<p><br><br><br><br><br><br><br><br><br></p>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "conda-env-571-py"
        },
        kernelOptions: {
            name: "conda-env-571-py",
            path: "./_lectures"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'conda-env-571-py'</script>

                </article>
              

              
              
                <footer class="bd-footer-article">
                  
<div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item"><!-- Previous / next buttons -->
<div class="prev-next-area">
</div></div>
  
</div>

                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#lecture-7-regularization">Lecture 7: Regularization</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-outcomes">Learning outcomes</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#motivation">1. Motivation</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#complex-models-and-the-fundamental-tradeoff">Complex models and the fundamental tradeoff</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#controlling-model-complexity">Controlling model complexity</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#idea-of-regularization-pick-the-line-hyperplane-with-smaller-slope">Idea of regularization: Pick the line/hyperplane with smaller slope</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#why-pick-the-line-hyperplane-with-smaller-slope">Why pick the line/hyperplane with smaller slope?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#regularization-l0-penalty-you-have-seen-before">Regularization: L0 penalty you have seen before</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Regularization: L0 penalty you have seen before</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#reminder-l0-l1-and-l2-norms">Reminder: L0, L1, and L2 norms</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#terminology-and-notation-l0-l1-and-l2-norms">Terminology and notation: L0, L1, and L2 norms</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#l2-regularization">2. L2 regularization</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#l2-penalty">L2 penalty</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">L2 regularization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#have-we-seen-this-before">Have we seen this before?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#size-of-regression-weights-and-overfitting">Size of regression weights and overfitting</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">Size of regression weights and overfitting</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#why-are-small-weights-better">Why are small weights better?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">Why are small weights better?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#how-alpha-affects-the-weights">How <code class="docutils literal notranslate"><span class="pre">alpha</span></code> affects the weights</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#l2-regularization-shrinking-example">L2-Regularization “Shrinking” Example</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#alpha-values-and-fit-of-the-model"><code class="docutils literal notranslate"><span class="pre">alpha</span></code> values and fit of the model</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-weights-become-smaller-but-never-become-zero">The weights become smaller but never become zero</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#regularization-path">Regularization path</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#question-l0-versus-l2-regularization-solution">Question: L0 versus L2 regularization (solution)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#how-to-pick-lambda">How to pick <span class="math notranslate nohighlight">\(\lambda\)</span>?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#should-we-regularize-the-y-intercept">Should we regularize the y-intercept?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#some-properties-of-l2-regularization">Some properties of L2 regularization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#interim-summary-l2-regularization">Interim summary: L2 regularization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#l1-regularization">3. L1 regularization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id5">L1-regularization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#similarities-with-l2-regularization">Similarities with L2-regularization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#terminology-and-notation-ridge-and-lasso">Terminology and notation: Ridge and Lasso</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-weights-become-smaller-and-smaller-and-become-zero">The weights become smaller and smaller and become zero</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#terminology-and-notation-sparsity">Terminology and notation: Sparsity</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#similarities-between-l1-and-l0-regularization">Similarities between L1- and L0-regularization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-l0-vs-l1-vs-l2">Example: L0 vs. L1 vs. L2</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#which-one-would-you-choose-with-l0-regularization">Which one would you choose with L0 regularization?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#which-one-would-you-choose-with-l1-regularization">Which one would you choose with L1 regularization?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#which-one-would-you-choose-with-l2-regularization">Which one would you choose with L2 regularization?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#optional-sparsity-and-regularization">(Optional) Sparsity and Regularization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#optional-sparsity-and-regularization-with-d-1">(optional) Sparsity and Regularization (with d=1)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#some-properties-of-l1-regularization">Some properties of L1 regularization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#l1-for-feature-selection">L1 for feature selection</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#feature-selection-using-l1-regularization">4. Feature selection using L1 regularization</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#feature-selection-methods-so-far">Feature selection methods so far</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#our-usual-game-of-getting-feature-names-for-the-coefficients">Our usual game of getting feature names for the coefficients</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#regularized-logistic-regression">Regularized logistic regression</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#logistic-regression-with-regularization">Logistic regression with regularization</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#regularization-scaling-and-colinearity">5. Regularization: scaling and colinearity</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#regularization-and-scaling">Regularization and scaling</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#scaling-features">Scaling features</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#normalizing-linearregression">Normalizing LinearRegression</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#scaling-summary">Scaling: summary</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#collinearity-and-regularization">Collinearity and regularization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#elastic-nets">Elastic nets</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#how-to-use-regularization-with-scikit-learn-some-examples">How to use regularization with <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code>: some examples</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#summary-l-regularization">Summary: L* regularization</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id6">Summary: L* regularization</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#true-false-questions-l2-regularization">True/False questions: L2-regularization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#true-false-questions-l1-regularization">True/False questions: L1-regularization</a></li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Varada Kolhatkar and Joel Östblom
</p>

  </div>
  
  <div class="footer-item">
    
  <p class="copyright">
    
      © Copyright 2022.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=e353d410970836974a52"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>